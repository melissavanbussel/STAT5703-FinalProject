---
title: "Final Project - Audit Data"
author: "Olivier Chabot"
date: "24/03/2020"
output:
    pdf_document:
    toc: true
theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Statement of Problem

Our audit dataset contains characteristics of 2000 individual tax returns. The objective is to predict the binary (TARGET_Adjusted) and continuous (RISK_Adjustment) target variables. The variables are:

* __ID__ (Unique identifier for each person)

* __Age__ (Age of person)

* __Employment__ (Type of employment)

* __Education__ (Highest level of education)

* __Marital__ (Current marital status)

* __Occupation__ (Type of occupation)

* __Outcome__ (Amount of income declared)

* __Gender__ (Gender of person)

* __Deductions__ (Total amount of expenses that a person claims in their financial statement)

* __Hours__ (Average hours worked on a weekly basis)

* __Risk_Adjustment__ (The continuous target variable; this variable records the monetary amount of any adjustment to the person’s financial stuatus as a result of a productive audit.  This variable is a measure of the size of the risk associated with the person)

* __TARGET_Adjusted__ (The binary target variable for classification modeling (0/1), indicating nonproductive and productive audits, respectively. Productive audits are those that result in an adjustment being made to a client’s financial statement.) 

# Data Exploration & Visualization

```{r}

# Loading the data set

audit <- read.csv("audit.csv")

# Loading libraries

library(ggplot2) # For basics graphs

library(GGally) # For scatter plot matrix

library(dplyr) # To manipulate and explore data


# Looking at the structure of our data

glimpse(audit)

```

Note that "Absent" is a possible answer for marrital status. According to https://www.census.gov/prod/2003pubs/c2kbr-30.pdf:

"Marital status: The marital status classification refers to the status
on the census date, April 1, 2000. The “now married” category
includes those who were “married, spouse present” and those who
were “married, spouse absent.” These latter two subcategories were
determined in the processing and editing steps by the presence or
absence of a spouse in the household as ascertained from the relationship-to-householder question on the long form and the assignment of people to related subfamilies. “Married, spouse present”
applies to husbands and wives if both were living in the same household. “Married, spouse absent” applies to husbands and wives who
answered that they were “Now married” on the census form but no
spouse could be found who could be linked to them in the editing
stages. Since people in group quarters housing (for example, institutions or shelters) were not asked the relationship item, all people in
group quarters housing who reported that they were “Now married”
were subsequently assigned to the “Married, spouse absent” category
in the recoding steps. "

Thus, absent would be someone who is married but living away (in a nursing home). Married but spouse absent would be a person who is married their their spouse is living somewhere else.

The unique identifier variable is irrevelant to our analysis. Employment, Occupation, Marital are all nominal categorical variables with lots of levels and thus can be ignored for the scatterplot matrix. Education however can be considered as an ordinal variable.

```{r}

levels(audit$Education)

```

We might assign values to each education level according to the following hierarchy:

* Doctorate (13)

* Master/Professional (12)

* Bachelor (11)

* Associate (10)

* College/Vocational (9)

* High school graduate (8)

* Grade 12 (7)

* Grade 11 (6)

* Grade 10 (5)

* Grade 9 (4)

* Grade 7 to 8 (3)

* Grade 5 to 6 (2)

* Grade 1 to 4 (1)

* Preschool (0)


```{r}
# Ordinal Education

edu_levels <- as.character(levels(audit$Education))
edu_rank <- c(10, 11, 9, 13, 8, 12, 0, 12, 9, 5, 6, 7, 1, 2, 3, 4)

audit$Education <- as.character(audit$Education)

class(audit$Education)

for (i in 1:16){

  for (j in 1:2000){
    if (audit[j, 4] == edu_levels[i]){
      audit[j, 4] <- edu_rank[i]
    }

  }

}

head(audit, n = 10)[ , 1:8]

audit$Education <- as.factor(audit$Education)

class(audit$Education)
```

We can use a scatterplot matrix to get an overview of our reduced data.

```{r}

 
# Scatterplot matrix

# base r

pairs(audit[ , -c(1, 3, 5, 6)], col = audit$TARGET_Adjusted + 4)

```

Some clusters seem to be present(Age, Deductions)|(Income, deduction)|(education hours)|(Hours, deductions)

```{r, eval = FALSE}
# ggplot version

audit$TARGET_Adjusted <- as.factor(audit$TARGET_Adjusted)

# Check correlations (as scatterplots), distribution and print corrleation coefficient
ggpairs(audit[ , -c(1, 3, 5, 6)], ggplot2::aes(colour = audit$TARGET_Adjusted), 
        title="correlogram with ggpairs()")
```




```{r}
# Class version
source("pairs_ext.r")

pairs(audit[ , -c(1, 3, 5, 6)], upper.panel = panel.cor, diag.panel = panel.hist)

audit <- read.csv("audit.csv")
```

The average number of hours worked on a weekly basis looks weird as well. Let's take a look at it.

```{r}
plot_multi_histogram <- function(df, feature, label_column) {
    plt <- ggplot(df, aes(x=eval(parse(text=feature)), fill=eval(parse(text=label_column)))) +
    geom_histogram(alpha=0.7, position="identity", aes(y = ..density..), color="black") +
    geom_density(alpha=0.7) +
    geom_vline(aes(xintercept=mean(eval(parse(text=feature)))), color="black", linetype="dashed", size=1) +
    labs(x=feature, y = "Density")
    plt + guides(fill=guide_legend(title=label_column))
}

audit$TARGET_Adjusted <-as.factor(audit$TARGET_Adjusted)
class(audit$TARGET_Adjusted)

plot_multi_histogram(audit, 'Hours','TARGET_Adjusted')

```

```{r}

# Boxplot of Hours

ggplot(data = audit, aes(y = audit$Hours)) +
  geom_boxplot()

# Relationship between outlier hours and income?

ggplot(data = audit, aes(x = audit$Hours, y = audit$Income)) +
  geom_point() + 
  scale_x_continuous(name="Hours") +
  scale_y_continuous(name="Income",
                     labels = scales::comma)
```

100 hour work-week is not impossible dig deeper into those cases. Maybe mistakes. We care more about the people who work __long hours__ and __do not__ report a high income. 

```{r}
audit <- read.csv("audit.csv")

audit[which((audit$Hours > 60) & (audit$Income < 30000)), -c(1, 2, 5, 8)]
```

A risk adjustment of 99 999$ is suspicious. Perhaps an error?


We can take a look at the summary of our data to d
```{r}
# Looking for outliers

summary(audit)
```

Note that __Employment__ has 100 NA's while __Occupation__ has 101 NA's. 

```{r}
which(is.na(audit$Employment) != is.na(audit$Occupation))
which(audit$Employment == "Unemployed")
```

Row 50 has an NA for __Occupation__ but it has "unemployed" for the __Employment__ variable. The NA for Occupation is a really a Not Applicable instead of a missing value.

Also note that there is only one observation in the entire dataset for which the __Employment__ variable is "Volunteer". We will change this to "Unemployed". 

```{r}
(volunteer <- which(audit$Employment == "Volunteer"))
audit$Employment[volunteer] <- "SelfEmp"
```

A few risk adjustments are negative.

```{r}
which(audit$RISK_Adjustment < 0)

audit[which(audit$RISK_Adjustment < 0), -c(1, 2, 5, 8)]
```

Nothing suspicious here. Let's look at the risk adjustment variable.


```{r}

which(audit$RISK_Adjustment > 50000)

reduced_audit <- audit[which(audit$RISK_Adjustment > 50000), ]

audit[which(audit$RISK_Adjustment > 50000), -c(1, 2, 5, 8)]

ggplot(data = reduced_audit, aes(x = reduced_audit$Income,
                                 y = reduced_audit$RISK_Adjustment,
                                 size = reduced_audit$Hours,
                                 colour = reduced_audit$Education)) +
  geom_point()
```

There a many adjustments of 99 999. Let's investigate further...

### Normalizing the data

```{r}
f.data.std <- function(data) {
data <- as.matrix(data) 
bar <- apply(data, 2, mean) 
s <- apply(data, 2, sd) 
t((t(data) - bar)/s)
}
```

```{r}
Age_N <- f.data.std(audit$Age)
Income_N <- f.data.std(audit$Income)
Deductions_N <- f.data.std(audit$Deductions)
Hours_N <- f.data.std(audit$Hours)
Risk_N <- f.data.std(audit$RISK_Adjustment)
audit_N <- cbind(audit, Age_N, Income_N, Deductions_N, Hours_N, Risk_N)

head(audit_N, n = 10)
```





# Dimension Reduction

# Data Reduction

# Unsupervised Learning - Clustering

# Supervised Learning - Classification and Modelling 

## Set-Up: Training and Test Set

Before we begin supervised learning, we should split our data into a training and test set so that we can evaluate the performance of our models. We will also re-normalize our numeric predictor variables, within the training set and testing set separately. In the real world, if we only had a training set, we would not have the test set to inform our normalization, so it is essential to normalize them separately if we want to accurately measure the performance of our models. 

Once the training and test sets have been created, it is important to examine the distribution of the classes (and the response variable) between the training and test set. Since there are very few productive audits in our dataset, we need to ensure that both productive and nonproductive audits are represented adequately in both the training set and the test set. The following plots show the distributions of the response variables of interest, using an 80 / 20 split for training / test size.

```{r}
# Set seed for reproducibility
set.seed(12)

# Create test training split 
n <- nrow(audit)
ntrain <- round(0.8 * n, 0)
ntest <- nrow(audit) - ntrain 
train_indices <- sample(1:n, size = ntrain, replace = FALSE)
train_audit <- audit[train_indices, ]
test_audit <- audit[-train_indices, ]

# Normalize numeric predictor variables, separately between training and test sets
train_audit$Age <- f.data.std(train_audit$Age)
train_audit$Income <- f.data.std(train_audit$Income)
train_audit$Deductions <- f.data.std(train_audit$Deductions)
train_audit$Hours <- f.data.std(train_audit$Hours)
test_audit$Age <- f.data.std(test_audit$Age)
test_audit$Income <- f.data.std(test_audit$Income)
test_audit$Deductions <- f.data.std(test_audit$Deductions)
test_audit$Hours <- f.data.std(test_audit$Hours)

# Convert classification variable to a factor variable 
train_audit$TARGET_Adjusted <- as.factor(train_audit$TARGET_Adjusted)
test_audit$TARGET_Adjusted <- as.factor(test_audit$TARGET_Adjusted)

# Create dataframe to be used for modelling the RISK_Adjustment variable
# Remove ID variable, and TARGET_Adjusted variable
train_audit_m <- train_audit[, -c(1, 12)]
test_audit_m <- test_audit[, -c(1, 12)]
# Create dataframe to be used for classifying the TARGET_Adjusted variable
# Remove the ID variable, and RISK_Adjustment variable
train_audit_c <- train_audit[, -c(1, 11)]
test_audit_c <- test_audit[, -c(1, 11)]

# Visualize the distribution of classes (and response) between training / test sets
par(mfrow = c(2, 2))
barplot(prop.table(table(train_audit$TARGET_Adjusted)), main = "Training set",
        xlab = "TARGET_Adjusted", ylab = "Proportion of observations")
barplot(prop.table(table(test_audit$TARGET_Adjusted)), main = "Test set", 
        xlab = "TARGET_Adjusted", ylab = "Proportion of observations")
hist(train_audit$RISK_Adjustment, probability = TRUE, main = "Training set", 
     xlab = "RISK_Adjustment", ylab = "Proportion of observations",
     xlim = c(0, 120000), col = "grey")
hist(test_audit$RISK_Adjustment, probability = TRUE, main = "Test set", 
     xlab = "RISK_Adjustment", ylab = "Proportion of observations",
     xlim = c(0, 120000), col = "grey")
```

The distributions look approximately equal, and we can proceed.

## Classification 

### k-NN (k Nearest Neighbours)

A simple classification method is *k Nearest Neighbours.* In this method, the $k$ closest vectors in the training set (in terms of Euclidean distance) are found for each observation in the test set. The classification of an observation in the test set is then decided by performing a "vote" using the $k$ closest vectors. If there are any ties, the ties are broken at random. Additionally, if there are ties in terms of finding the $k^{th}$ nearest vector, all of the candidate vectors are included in the "voting" process. We will be using the `class` package to perform the classification.

Since we are dealing with the Euclidean distance, we will have to consider only our numeric variables in this case.

In the figure below, we plot the proportion of correctly classified observations in the test set as a function of the value of $k$. 

```{r}
# Load library for performing kNN
library(class)

# Set seed for reproducibility 
set.seed(19)

# Get just the numeric variables 
audit_num_train <- train_audit_c[, c(1, 6, 8:9), ]
audit_num_test <- test_audit_c[, c(1, 6, 8:9), ]

# Try varying values of k 
k <- 1:50

# Perform kNN 
knn_test_acc <- vector(length = 50)
for (i in 1:length(knn_test_acc)) {
  knn_audit <- knn(train = audit_num_train, test = audit_num_test, 
                 cl = train_audit_c$TARGET_Adjusted, k = k[i])
  knn_test_acc[i] <- sum(knn_audit == test_audit_c$TARGET_Adjusted) / ntest
}

# Create plot of results
plot(k, knn_test_acc, main = "Test accuracy of k Nearest Neighbours", 
     xlab = "k", ylab = "Test set accuracy", type = "l")
```

From the plot, we can see that the value of $k$ which resulted in the highest classification accuracy on the test set was $k = 22$, resulting in an accuracy of 79%.

### Linear and Quadratic Discriminant Analysis

*Discriminant analysis* is a classification technique that uses the training data to determine the boundaries which best separate the classes. We treat the observations of each class as observations from a multivariate normal (MVN) distribution, and compute the boundaries using a formula which depends on the parameters of the MVN distributions. If we assume that the covariance matrices of all the MVN distributions are the same, we perform *Linear Discriminant Analysis* (LDA) and the resulting boundaries are lines. If we relax this assumption, we peform *Quadratic Discriminant Analysis* (QDA) and the resulting boundaries are quadratic. LDA is computationally easy to perform, but linear boundaries may not always be appropriate. QDA gives us more freedom with our boundaries, at the expense of taking longer to perform. 

We will use the `MASS` package to perform the discriminant analysis. This package requires an option to be specified for missing values. We will simply omit the missing values in the calculation.

```{r}
# Load library
library(MASS)

# Perform LDA 
audit_lda <- lda(formula = TARGET_Adjusted ~ ., data = train_audit_c[, c(1, 6, 8:10)],  
                 na.action = na.omit)
lda_predict <- predict(audit_lda, test_audit_c)
lda_test_acc <- sum(lda_predict$class == test_audit_c$TARGET_Adjusted) / ntest 

# Perform QDA
audit_qda <- qda(formula = TARGET_Adjusted ~ ., data = train_audit_c[, c(1, 6, 8:10)],  
                 na.action = na.omit)
qda_predict <- predict(audit_qda, test_audit_c)
qda_test_acc <- sum(qda_predict$class == test_audit_c$TARGET_Adjusted) / ntest 
```

With LDA, our proportion of correctly classified test observations is 0.8, while it is 0.7925 with QDA. Although this may make it seem like LDA is the superior method, this is not the case -- LDA is a subset of QDA, and thus should QDA be able to obtain the same results as LDA given a large enough training set that is representative of the testing set. 

Since we performed the discriminatory analysis on more than two numeric variables, we are unable to visualize the results the exact way we did in lecture. There will not be clear lines (or quadratic curves) that separate the class predictions *perfectly* on a 2-dimensional plot, since the calculations were completed in 4 dimensions. Instead, we can create plots of the different 2 variable combinations, using the `klaR` package. Note that the plots below are created for the training data, and do not include any information about the test data.

```{r}
# Load library 
library(klaR)

# Note: The partimat function calls the lda() and qda() functions from the MASS package
# Create plot of LDA results on each of the 2 variable combinations 
partimat(formula = TARGET_Adjusted ~ ., data = train_audit_c[, c(1, 6, 8:10)], 
         method = "lda", na.action = na.omit, main = "LDA on Audit data")

# Create plot of LDA results on each of the 2 variable combinations 
partimat(formula = TARGET_Adjusted ~ ., data = train_audit_c[, c(1, 6, 8:10)], 
         method = "qda", na.action = na.omit, main = "QDA on Audit data")
```

### Classification trees

One way of classifying our observations into productive and nonproductive audits is through the use of classification trees. There are many ways in R to create classification trees, but we will start by using the `tree` package. The figures below show the resulting classification trees on the training dataset, using the deviance (first tree) and the generalized gini index (second tree) as the measures. Note that the labels have been turned off in the second plot, as the plot of the second tree becomes unreadable otherwise (highly concentrated at the bottom of the tree, implying that the decrease in impurity is fairly low in the bottom branches -- pruning may be necessary). 

```{r, message = FALSE}
# Load libraries for creating decision trees
library(tree)

# Use deviance:
# Note: mincut is the minimum number of observations per node
tree_dev <- tree(formula = TARGET_Adjusted ~., data = train_audit_c,
                 split = "deviance", mincut = 5)
# Use generalized gini
tree_gini <- tree(formula = TARGET_Adjusted ~., data = train_audit_c,
                  split = "gini", mincut = 5)

# Load Dr. Mills' code
source("ejnplottree.r")
source("ejntexttree.r")
source("EJNPartitionTree.r")

# Visualize the results 
plot.tree.EJN(tree_dev)
text.tree.EJN(tree_dev, all = TRUE)
plot.tree.EJN(tree_gini)

# Calculate prediction accuracy on the test set 
tree_dev_predict <- predict(tree_dev, test_audit_c, type = "class")
tree_gini_predict <- predict(tree_gini, test_audit_c, type = "class")
prop_correct_dev <- length(which(tree_dev_predict == test_audit_c$TARGET_Adjusted)) / ntest
prop_correct_gini <- length(which(tree_gini_predict == test_audit_c$TARGET_Adjusted)) / ntest
```

Using the proportion of correctly classified observations in the test set as a measure of model accuracy, the above models had accuracies on the test set of 86.5% and 82.5% respectively.

One problem with the plotting that we can see immediately is that the `tree` package does not plot categorical variables very well. Rather than displaying the names of categories, the plot displays a concatenation of letters corresponding to the positions of the level names. For example, the first split on the tree on the left is based on the `Marital` vlariable. The plot includes the letters "abdef" on this split, which actually means levels 1, 2, 4, 5 and 6 of the `Marital` variable (corresponding to the categories "Absent", "Divorced", "Married-spouse-absent", "Unmarried" and "Widowed"). 

In order to create a plot that is a bit more clear about the decision rules, we can use the `rpart` and `rpart.plot` packages. We note that the trees may appear slightly different between the `tree` package implementation and the `rpart` package implementation even if the trees are fairly similar (the `rpart` package does not always create decision rules in the form *var < split*). The plot below shows the classification tree created by the `rpart` package, using the entire dataset. We have used the same value for the minimum number of observations per node as we used in the previous two plots: 

```{r}
# Load libraries for creating more visually appealing classification trees
library(rpart)
library(rattle)
library(RColorBrewer)
library(rpart.plot)

# Create decision tree
# Note: minsplit is the minimum number of observations per node
tree_rpart <- rpart(formula = TARGET_Adjusted ~., data = train_audit_c,
                    method = "class", minsplit = 5)

# Plot the tree
fancyRpartPlot(tree_rpart, sub = "Classification tree created using the rpart package")
```

Here, we can see that the plot becomes much more readable. For example, the first node now shows us which levels of the `Marital` variable correspond with going left or right down the tree. 

#### Pruning 

*Pruning* classification trees can help to prevent overfitting, removing the least important splits from our tree. The `cv.tree` function will perform K-fold cross validation for us, determining the best splits to prune. 

The plot below shows the size of the tree versus the deviance (using 10-fold cross validation) for the two trees created using the `tree` package, and we can see that the lowest deviance occurs when the size of the tree is 4 for the first tree, and when the size is 5 for the second tree. 

```{r}
# Perform 10-fold CV 
tree_cv_dev <- cv.tree(tree_dev, FUN = prune.tree, K = 10)
tree_cv_gini <- cv.tree(tree_gini, FUN = prune.tree, K = 10)

# Plot the size of the tree versus the deviance
par(mfrow = c(1, 2))
plot(tree_cv_dev)
plot(tree_cv_gini)
```

We can now use the results of this cross validation to visualize the trees that occurs when we prune them as per the suggested sizes: 

```{r}
# Use the results of the cross validation to plot the pruned trees
par(mfrow = c(1, 2))
pruned_tree_dev <- prune.misclass(tree_dev, best = 4)
plot.tree.EJN(pruned_tree_dev)
text.tree.EJN(pruned_tree_dev, all = TRUE)
pruned_tree_gini <- prune.misclass(tree_gini, best = 5)
plot.tree.EJN(pruned_tree_gini)
text.tree.EJN(pruned_tree_gini, all = TRUE)
```

Now, the trees look much cleaner. The accuracy on the test set has also increased: 

```{r}
pruned_tree_dev_pred <- predict(pruned_tree_dev, test_audit_c, type = "class")
pruned_tree_gini_pred <- predict(pruned_tree_gini, test_audit_c, type = "class")
prop_correct_dev_prune <- length(which(pruned_tree_dev_pred == test_audit_c$TARGET_Adjusted)) / ntest
prop_correct_gini_prune <- length(which(pruned_tree_gini_pred == test_audit_c$TARGET_Adjusted)) / ntest

# Create table to display the results
results <- data.frame(Original = c(prop_correct_dev, prop_correct_gini),
                      Pruned = c(prop_correct_dev_prune, prop_correct_gini_prune))
row.names(results) <- c("Deviance", "Gini")

# Display the results
library(kableExtra)
kable(results) %>% 
  kable_styling(latex_options = "striped")
```

We can see that the pruning had a positive impact on the second tree, reducing overfitting and improving the test accuracy. 

#### Bagging 

*Bagging*, or Boostrap Aggregating, is a method we can use to improve the stability of algorithms (such as classification trees). 

Within our training data, we will do the following a total of 100 times: 

- Split the data into a train and test set; let the size of the train set be $N$
- Create 50 bootstrap replicates (creating a sample of size $N$ by sampling from the training set with replacement)
- Create a tree for each of the 50 bootstrap replicates, pruning as needed
- Use the 50 trees to "vote" on the correct classes for each observation in the test set

At the end, the 100 results will be used to perform another vote. 

```{r}
# Set seed for reproducibility 
set.seed(2020)

# Create a function to get the most commonly occurring value in a vector
# We will use this to determine the "winner" of "votes"
# Do not consider 0s as this will give us problems when combining the 100 votes at the end
getmode <- function(x) {
   uniqx <- unique(x)
   if (0 %in% uniqx == FALSE) {
     return(uniqx[which.max(tabulate(match(x, uniqx)))])
   } else {
     x <- x[x != 0]
     uniqx <- unique(x)
   }
   return(uniqx[which.max(tabulate(match(x, uniqx)))])
}
```

```{r, eval = FALSE}
# Size of our dataset 
N <- round(0.9 * ntrain, 0)
# Number of bootstrap replicates we will take at each iteration
sz <- 50
# Create a matrix to store our results
bag <- matrix(0, nrow = ntrain, ncol = 100)
# Create a list to store all of our trees
rep.trees <- vector(mode = "list", length = 100)

# Perform the bagging 
for (j in 1:100) {
  # Create a train / test split (use 90 / 10)
  temp <- 1:ntrain 
  train_idx <- sample(temp, size = N
                      , replace = FALSE)
  test_idx <- temp[temp %in% train_idx == FALSE]
  train <- train_audit_c[train_idx, ]
  test <- train_audit_c[test_idx, ]
  size_train <- N
  size_test <- ntrain - size_train
  
  # Create the bootstrap replicates and vote 
  replicates <- matrix(0, nrow = sz, ncol = N)
  rep.trees[[j]] <- vector(mode = "list", length = sz)
  pred.tree <- matrix(0, nrow = size_test, ncol = sz)
  for (i in 1:sz) {
    # Create 50 bootstrap replicates by sampling (w replacement) from original training set
    replicates[i, ] <- sample(1:N, size = N, replace = TRUE)
    
    # Create trees for each of the 50 bootstrap replicates
    rep.trees[[j]][[i]] <- rpart(formula = TARGET_Adjusted ~ ., 
                                 data = train[replicates[i, ], ])
    
    # Use the 50 trees to predict the classes of the test observations
    pred.tree[, i] <- predict(rep.trees[[j]][[i]], test, type = "class")
  }

  # Vote on the classes of the test observations, using the predictions from the 50 trees
  bag[test_idx, j] <- apply(pred.tree, 1, getmode)
}

# Save results in a file 
save(bag, file = "bag.RDa")
save(rep.trees, file = "rep.trees.RDa")
```

```{r}
# Load the saved files
load("bag.RDa")
load("rep.trees.RDa")

# Vote on the classes of the observations, using the 100 iterations 
bag.class <- apply(bag, 1, getmode)

# Check the training accuracy 
train_acc_bag <- sum(bag.class == as.numeric(train_audit_c$TARGET_Adjusted)) / ntrain 

# Use the models to predict the classes of the test set
bag_test <- matrix(0, nrow = ntest, ncol = 100)
for (j in 1:100) {
  temp_predictions <- matrix(NA, nrow = ntest, ncol = 50)
  for (i in 1:50) {
    temp_predictions[, i] <- as.numeric(predict(rep.trees[[j]][[i]], 
                                                      test_audit_c, type = "class"))
  }
  bag_test[, j] <- apply(temp_predictions, 1, FUN = getmode)
}
# Vote on the classes of observations in test_audit_c, using the 100 iterations
bag_test_class <- apply(bag_test, 1, FUN = getmode)

# Check the prediction accuracy on the test set
test_acc_bag <- sum(bag_test_class == as.numeric(test_audit_c$TARGET_Adjusted)) / ntest
```

Using the bagging, we obtain a training accuracy (on our 1600 observations) of 83.0625%. Using the models from the bagging on our reserved test set (400 observations), we obtain a test accuracy of 88.25%, which is a huge improvement! The bagging was highly successful.

#### Random Forests

*Random Forests* are an extension of bagging, where features are also randomly selected. Although we did not cover this topic during lecture, random forests are a very popular technique and are easy to create in R, so we will try a simple model. 

Note that with the `randomForest` function in R, we need to choose a way of handling missing data. Here, we will simply omit the incomplete observations. 

When using the `randomForest` function in R, there is one main parameter to tune: `mtry`. This is the number of variables that are randomly sampled as candidates at each split. The plot below shows the model accuracy on the test set for varying values of `mtry`: 

```{r}
# Load library for creating random forests
library(randomForest)

# Set seed for reproducibility 
set.seed(41)

# Since we have 9 predictor variables, we will vary the value of mtry from 1 to 9
mtry <- 1:9

# Create the random forests, store the accuracy on the test set 
rf_test_acc <- vector(length = 9)
for (i in 1:9) {
  rf <- randomForest(formula = TARGET_Adjusted ~., data = train_audit_c, 
                     na.action = na.omit, mtry = mtry[i], ntree = 500)

  # Predict the classes of the test set using the random forest model 
  rf_predict <- predict(rf, newdata = test_audit_c)

  # Compute the accuracy on the test set 
  matches <- rf_predict == test_audit_c$TARGET_Adjusted
  rf_test_acc[i] <- sum(matches, na.rm = TRUE) / length(which(is.na(matches) == FALSE)) 
}

plot(mtry, rf_test_acc, type = "l", main = "Test Accuracy using Random Forests", 
     xlab = "Value of mtry", ylab = "Test accuracy (percent)")
```

The highest accuracy occurred when `mtry` was set to 4. In this case, the prediction accuracy on the test set was 87.2%. While this is a bit lower than what we obtained with bagging, it should be noted that the computational time for this method was significantly lower than it was for bagging. 

## Modelling 

### Linear models

We can start with Ordinary Least Squares regression: 

```{r}
# Set seed for reproducibility 
set.seed(23)

# Create test training split 
n <- nrow(audit)
train_indices <- sample(1:n, size = round(0.8 * n, 0), replace = FALSE)
train_audit <- audit[train_indices, ]
test_audit <- audit[-train_indices, ]

# Using just training data: 
# Start with just numeric variables
lmod1 <- lm(RISK_Adjustment ~ Income + Deductions + Hours, data = train_audit)
# Predict on test: 

lmod1_predict <- predict(lmod1, test_audit)

# Just numeric variables, without intercept term 
lmod2 <- lm(RISK_Adjustment ~ Income + Deductions + Hours - 1, data = train_audit)
# Predict on test: 
lmod2_predict <- predict(lmod2, test_audit)

# Create table comparing AIC, adjusted R^2 of various models, % diff on predictions
results_train <- data.frame(R2 = c(summary(lmod1)$adj.r.squared, summary(lmod2)$adj.r.squared),
                            AIC = c(AIC(lmod1), AIC(lmod2)), 
                            avg_percent_diff = c(mean(lmod1_predict - test_audit$RISK_Adjustment),
                                             mean(lmod2_predict - test_audit$RISK_Adjustment)))

# Using all data:
# Start with just numeric variables
lmod1 <- lm(RISK_Adjustment ~ Income + Deductions + Hours, data = audit)

# Just numeric variables, without intercept term 
lmod2 <- lm(RISK_Adjustment ~ Income + Deductions + Hours - 1, data = audit)

# All predictor variables
lmod1 <- lm(RISK_Adjustment ~ Age + Employment + Education + Marital + Occupation + 
              Income + Gender + Deductions + Hours, data = audit)

# Create table comparing AIC, adjusted R^2 of various models 
results_train <- data.frame(R2 = c(summary(lmod1)$adj.r.squared, summary(lmod2)$adj.r.squared),
                            AIC = c(AIC(lmod1), AIC(lmod2)),
                            MSE = c(sum(lmod1$residuals^2), sum(lmod2$residuals^2)))

# Look at some diagnostic plots (neither meet the assumptions clearly)
par(mfrow = c(1, 2))
plot(lmod1, which = 2)
plot(lmod2, which = 2)

# Look at observation 1551 (outlier from the previous plot)
# Only declared 23800, risk adjustment of 112243??? 
audit[1551, ]
```

### MARS

*Multivariate Adaptive Regression Splines*, commonly known as MARS, is a non-parametric way of modelling data using a series of piecewise linear regression models. 

While the `mda` package in R offers a `mars` function to perform MARS, there is a 2019 package called `earth` which uses cross validation to find the optimal placements of knots across all provided predictor variables, and prunes accordingly. The function continues searching until the change in $R^2$ is less than a specified threshold. 

This package also works well the `caret` package for further tuning of parameters. The main parameters that we wish to tune when using MARS are `degree` and `nprune`. The `degree` parameter controls the maximum degree of interaction terms that are allowed in the model, and the `nprune` parameter controls the maximum number of predictors that are allowed in the model. We will use the `caret` package to perform 10-fold cross validation to pick the best values for `degree` and `nprune`. The `caret` package recommends using RMSE (root mean squared error) as the selection criterion for parameter tuning when the response variable is continuous.  

```{r}
# Load packages for performing MARS and for tuning parameters
library(earth)
library(caret)

# Keep only the observations that have no missing values 
# since the `earth` function cannot handle missing values
complete_train_audit_m <- train_audit_m[complete.cases(train_audit_m), ]

# Create a parameter tuning "grid" 
parameter_grid <- floor(expand.grid(degree = 1:4, nprune = seq(2, 50, by = 5)))

# Set seed for reproducibility
set.seed(2020)
```

```{r, eval = FALSE}
# Perform cross-validation
cv_mars_audit <- train(x = complete_train_audit_m[, 1:9], 
                       y = complete_train_audit_m[, 10], 
                       method = "earth",
                       metric = "RMSE",
                       trControl = trainControl(method = "cv", number = 10),
                       tuneGrid = parameter_grid)
# Save the results 
save(cv_mars_audit, file = "cv_mars_audit.RDa")
```

```{r}
# Load files 
load("cv_mars_audit.RDa")

# Visualize the results from the cross validation on the parameter tuning 
ggplot(cv_mars_audit) + 
  labs(x = "Value of nprune, the max # of predictors allowed", 
       fill = "Value of degree, max degree of interaction allowed") +
  ggtitle("Parameter Tuning for MARS")
```

We can see that the root mean squared error is minimized when `degree` is 1 and `nprune` is 2. For all values of these parameters, we can see that this quantity is very high. When we consider that the largest value of `RISK_Adjustment` in our training set (our response variable) is 99999, we see that this value of RMSE is extremely high and suggests that our model does not fit the training data very well. 

```{r}
# Remove observations with missing values from the test set 
complete_test_audit_m <- test_audit_m[complete.cases(test_audit_m), ]

# Use the best model to predict RISK_Adjustment for the test set 
mars_predict <- predict(cv_mars_audit$finalModel, newdata = complete_test_audit_m)

# Get the SSE 
mars_sse <- sum((complete_test_audit_m$RISK_Adjustment - mars_predict)^2)

# Get the mean absolute error 
mars_mae <- mean(abs(complete_test_audit_m$RISK_Adjustment - mars_predict))
```

We can use our model to predict the response for the test set. Doing this, we obtain an SSE of roughly 30.8 billion, which is extremely high but is still lower than what we obtained with OLS. On average, this works out to an error of roughly $2800. 

Below, we plot the residuals, and observe that the assumptions for linear regression are very clearly violated. In order for the MARS algorithm to be successful at modelling non-linear data, the data still must be linear enough in small sections that piecewise linear regression would be useful. This is **not** the case with our data, as evidenced by our residual plot.

```{r}
plot(complete_test_audit_m$RISK_Adjustment - mars_predict, 
     main = "Residuals for MARS model", 
     xlab = "Observation number in test set", 
     ylab = "Residual")
```

### Regression Trees

Regression trees are very similar to classification trees, with the exception that they aim to predict the value of a continuous variable rather than a binary variable. With regression trees, there are only specific values which may be predicted. These possible values are visible in the leaf nodes (bottom) of the tree.

First, we use the `tree` package, trying both the `deviance` and `gini` options. The `deviance` option results in a tree with 5 leaf nodes, but the `gini` option results in a tree which has only has a root. This indicates that the data may not be well suited to regression trees. The tree below shows the regression tree obtained using the `deviance` option in the `tree` function: 

```{r, message = FALSE}
# Use deviance:
# Note: mincut is the minimum number of observations per node
reg_tree_dev <- tree(formula = RISK_Adjustment ~ ., data = train_audit_m,
                     split = "deviance")

# Use generalized gini
reg_tree_gini <- tree(formula = RISK_Adjustment ~ ., data = train_audit_m,
                      split = "gini")

# Visualize the results 
plot(reg_tree_dev)
text(reg_tree_dev, all = TRUE)
# The gini tree will not plot, as it is only a root 
# plot(reg_tree_gini)
# text(reg_tree_gini, all = TRUE)

# Calculate prediction accuracy on the test set 
reg_tree_dev_predict <- predict(reg_tree_dev, test_audit_m)
reg_tree_gini_predict <- predict(reg_tree_gini, test_audit_m)
reg_tree_dev_sse <- sum((test_audit_m$RISK_Adjustment - reg_tree_dev_predict)^2)
reg_tree_gini_sse <- sum((test_audit_m$RISK_Adjustment - reg_tree_gini_predict)^2)
```

We can also use the `rpart` package to create regression trees. 

```{r}
# Create decision tree
# Note: minsplit is the minimum number of observations per node
reg_tree_rpart <- rpart(formula = RISK_Adjustment ~ ., data = train_audit_m)

# Calculate prediction accuracy on the test set 
reg_tree_rpart_predict <- predict(reg_tree_rpart, test_audit_m)
reg_tree_rpart_sse <- sum((test_audit_m$RISK_Adjustment - reg_tree_rpart_predict)^2)

# Plot the tree
fancyRpartPlot(reg_tree_rpart, sub = "Regression tree created using the rpart package")
```

We can see that the `rpart` tree had 6 leaf nodes.

For the `tree` package, the regression tree created using the `deviance` option had an SSE of approximately 30.8 billion when predicting the response variable on the test set, while it was 32.6 billion for the tree created using the `gini` option. This is not surprising to see a large difference between the two values, since we know that the second tree was simply predicting the same value regardless of the input. For the regression tree created using the `rpart` package, the SSE on the test set was ~32.7 billion. 

The plot below shows the residuals for the regression tree created using the `tree` package with the `deviance` option.

```{r}
plot(test_audit_m$RISK_Adjustment - reg_tree_dev_predict, 
     main = "Residuals for Regression Tree (Deviance)", 
     xlab = "Observation number in test set", 
     ylab = "Residual")
```

We can see that the residuals once again indicate that our model was not well suited to our data. 

While we cannot visualize in as many dimensions as there are in our dataset, we can choose one variable in our dataset and plot it again the response variable for both the true values and the predicted values using the regression tree: 

```{r}
plot(test_audit_m$Age, test_audit_m$RISK_Adjustment, pch = 19,
     main = "Regression Tree Predictions on Test Set (Using 'Deviance')", 
     xlab = "Normalized Age",
     ylab = "Risk Adjustment")
points(test_audit_m$Age, reg_tree_dev_predict, pch = 19, col = "red")
legend(legend = c("True", "Predicted"), col = c("Black", "Red"), pch = c(19, 19), 
       x = "topleft")
```

From the plot, we can see that our regression tree is failing to predict high risk adjustments. It is also failing to predict any risk adjustments of exactly 0.

# References

https://rstudio-pubs-static.s3.amazonaws.com/202050_60f5c6dd1b884ba3ac29f1aef5c848bb.html
