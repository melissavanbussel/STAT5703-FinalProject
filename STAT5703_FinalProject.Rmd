---
title: "STAT 5703 Final Project"
author: "Olivier Chabot, Richard Matzinger, Melissa Van Bussel"
date: "Due: April 7th, 2020"
output:
   pdf_document:
      fig_caption: true
      number_sections: true
---

\newpage 
\tableofcontents 
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

# Statement of Problem

Our audit dataset contains characteristics of 2000 individual tax returns. The objective is to predict the binary (`TARGET_Adjusted`) and continuous (`RISK_Adjustment`) target variables. The variables are:

* __ID__ (Unique identifier for each person)

* __Age__ (Age of person)

* __Employment__ (Type of employment)

* __Education__ (Highest level of education)

* __Marital__ (Current marital status)

* __Occupation__ (Type of occupation)

* __Income__ (Amount of income declared)

* __Gender__ (Gender of person)

* __Deductions__ (Total amount of expenses that a person claims in their financial statement)

* __Hours__ (Average hours worked on a weekly basis)

* __RISK_Adjustment__ (The continuous target variable; this variable records the monetary amount of any adjustment to the person's financial status as a result of a productive audit.  This variable is a measure of the size of the risk associated with the person)

* __TARGET_Adjusted__ (The binary target variable for classification modeling (0/1), indicating nonproductive and productive audits, respectively. Productive audits are those that result in an adjustment being made to a clientâ€™s financial statement)

\newpage

# Data Exploration & Visualization

Before we begin manipulating our data, we will perform exploratory data analysis to get a sense for the structure of the data we are working with.

First, we use the `skimr` package to see the variables and their data types: 

```{r}
# Load the data set
audit <- read.csv("audit.csv")

# Loading libraries
library(ggplot2) # For basics graphs
library(GGally) # For scatter plot matrix
library(dplyr) # To manipulate and explore data
library(skimr) # To skim the structure of the dataset

# Convert the binary response variable to a factor variable
audit$TARGET_Adjusted <- as.factor(audit$TARGET_Adjusted)

# Looking at the structure of our data
audit_skim <- skim_without_charts(audit)
audit_skim[ -c(11:13)]
```

We see that we have 6 categorical and 6 numeric variables. The numeric variables are on very different scales, indicating that normalization of the data will be necessary for modeling. We can also see a couple of class imbalances. With our binary response variable, only 463 out of 2000 individuals (or roughly 23%) resulted in productive audits. Additionally, significantly more males than females were sampled. The average income in our dataset is quite high, despite the fact that the minimum income is $610, suggesting that there may be influential outliers. We also notice that most people have low deductions, if any. 

The only variables with missing values are `Employment` and `Occupation`, with roughly 5% missingness. Note that __Employment__ has 100 `NA`s while __Occupation__ has 101 `NA`s. 

```{r, eval = FALSE}
# Which observation has an NA for Occupation, but not for Employment?
which(is.na(audit$Employment) != is.na(audit$Occupation))
which(audit$Employment == "Unemployed")
```

Row 50 has a missing value for the __Occupation__ variable but it has "Unemployed" for the __Employment__ variable. Therefore, the `NA` for Occupation should really be considered as "Not Applicable" rather than a missing value.

```{r}
unemployed <- which(audit$Employment == "Unemployed")
audit$Employment[unemployed] <- "Not Applicable"
```

Also note that there is only one observation in the entire dataset for which the __Employment__ variable is "Volunteer". We will change this to "SelfEmp", since this person does have an income.

```{r}
volunteer <- which(audit$Employment == "Volunteer")
audit$Employment[volunteer] <- "SelfEmp"
audit$Employment <- droplevels(audit$Employment) # drop unused levels (Volunteer)
```

A few (7) of the risk adjustments are negative. Perhaps negative adjustments reflect individuals who over-reported their income and thus paid more taxes than they were supposed to.

We note that "Absent" is a valid marital status. According to the United States Census Bureau [@census], "absent" refers to someone who is married but living away (e.g., in a nursing home). Similarly, "Married-spouse-absent" refers to a person who is married but whose spouse is living somewhere else.

We also see that the unique identifier variable is irrelevant to our analysis. `Employment`, `Occupation`, `Marital`, and `Gender` are all nominal categorical variables with many levels. `Education`, however, could be treated as an ordinal variable.

We might assign values to each education level according to the following hierarchy:

* Doctorate (13)

* Master/Professional (12)

* Bachelor (11)

* Associate (10)

* College/Vocational (9)

* High school graduate (8)

* Grade 12 (7)

* Grade 11 (6)

* Grade 10 (5)

* Grade 9 (4)

* Grade 7 to 8 (3)

* Grade 5 to 6 (2)

* Grade 1 to 4 (1)

* Preschool (0)

```{r}
# Converting the education levels to their numeric rank
edu_levels <- as.character(levels(audit$Education))
edu_rank <- c(10, 11, 9, 13, 8, 12, 0, 12, 9, 5, 6, 7, 1, 2, 3, 4)

# Create a separate dataframe with ordinal educational ranks
audit_edu <- audit
audit_edu$Education <- as.character(audit_edu$Education)

# Convert the education levels to numerical ranks 
for (j in 1:2000) {
  for (i in 1:16) {
    if (audit_edu[j, 4] == edu_levels[i]) {
      audit_edu[j, 4] <- edu_rank[i]
    }
  }
}
audit_edu$Education <- as.integer(audit_edu$Education)
```

We can use a scatterplot matrix to get an overview of our new data. We will exclude the categorical variables from this visualization.

```{r, warning = FALSE, message = FALSE}
# Scatterplot matrix using ggplot
ggpairs(audit_edu[, -c(1, 3, 5, 6)],
        cardinality_threshold = 20,
        ggplot2::aes(colour = audit_edu$TARGET_Adjusted),
        title = "Augmented Scatterplot Matrix of Audit Data") +
  theme(axis.text = element_text(size = 6, angle = 45))
```

Some rough clusters seem to be present between `Age` and `Deductions`, `Income` and `Deductions`, `Education` and `Hours`, `Hours` and `Deductions`. The light blue dots represent the cases with a productive audit.

Next, we create a ridgeline density plot to examine the distribution of the `Income` variable across the various education levels.

```{r}
# Load packages for creating ridgeline density plots
library(ggridges)
library(viridis)

# Create ridgeline density plot
ggplot(audit_edu,
       aes(x = Income, y = as.factor(Education),
           group = as.factor(Education), fill = ..x..)) +
  geom_density_ridges_gradient(rel_min_height = 0.01) +
  scale_fill_viridis(option = "plasma") +
  scale_x_continuous(name = "Income", labels = scales::comma) +
  labs(x = "Income", y = "Education Level") +
  theme(legend.position = "none")
```

The distribution of income seems to be skewed right with similar values for highest density across all levels of education. Surprisingly, the highest density income seems to decrease with increased levels of education after "High School Graduate". Perhaps there is some truth to the saying "you peaked in high school", or perhaps something else is going on. It is very odd that those with a "Preschool" level of education are making such high incomes. This certainly raises a red flag and may suggest that some of the data has either been miscoded, tampered with, or is just highly unusual. 

Next, we explore the distribution of the `Age` and `Income` variables, grouped by audit status (productive or nonproductive). 

```{r}
# Create function to plot grouped histograms with overlayed densities and mean
plot_multi_histogram <- function(df, feature, label_column) {
    plt <- ggplot(df, aes(x = eval(parse(text = feature)), 
                          fill = eval(parse(text = label_column)))) +
    geom_histogram(alpha = 0.4, position = "identity", 
                   aes(y = ..density..), color = "black") +
    geom_density(alpha = 0.4) +
    geom_vline(aes(xintercept = mean(eval(parse(text = feature)))), 
               color = "black", linetype = "dashed", size = 1) +
    labs(x = feature, y = "Density")
    plt + guides(fill = guide_legend(title = label_column))
}

# Load package for creating a grid of ggplots 
library(gridExtra)

# Create grouped histograms for the Age and Income variables
p1 <- plot_multi_histogram(audit_edu, "Income", "TARGET_Adjusted") + 
  theme(legend.position = "none")
p2 <- plot_multi_histogram(audit_edu, "Age", "TARGET_Adjusted") +
  labs(y = "")
grid.arrange(p1, p2, nrow = 1, widths = c(0.45, 0.55))
```

The plot on the left suggests that most individuals who resulted in productive audits had reported a low-to-average income. This could be another red flag, since we might expect that those with higher incomes are more likely to commit tax fraud. It is possible, though, that individuals with lower incomes are more likely to under report income in order to access social programs. The plot on the right suggests that more older individuals result in productive audits. 

Below, we visualize the relationship between annual income and the average number of hours worked per week, grouped by audit status. 

```{r}
# Plot relationship between annual income and average number of hours worked
ggplot(data = audit, aes(x = Hours, y = Income,
                         colour = TARGET_Adjusted)) +
  geom_point() +
  geom_smooth(method = "lm") +
  ggtitle("Relationship between income and hours worked") +
  scale_x_continuous(name = "Hours") +
  scale_y_continuous(name = "Income", labels = scales::comma)
```

It is odd that annual income is negatively correlated with the average number of hours worked, though this correlation is very weak. In the bottom right of the plot, we notice that many people who work __above average hours__ and __do not__ report a high income are often productive audits. 

Now we will take a closer look at the risk adjustment variable. In the plot below, we examine the relationship between `RISK_Adjustment` and `Income` for the productive audits. We colour the points pink for females and blue for males, and set the size of the points proportional to the average number of hours worked per week. Additionally, we identify any individuals which have an income of less than $30,000 but who work more than 60 hours per week (the black points on the plot). Finally, we add a dashed line at `RISK_Adjustment = 99999`, showing that this is a popular value in the dataset. There is only one risk adjustment which is above this value, which makes us question whether this point is simply an outlier or if the value was miscoded by mistake. 

```{r}
# Create data frame containing the non-zero risk adjustments 
nonzero_adj <- audit[which(audit$RISK_Adjustment > 0), ]

# Plot the relationship between 
ggplot(data = nonzero_adj, 
       aes(x = Income, y = RISK_Adjustment, size = Hours, colour = Gender)) +
  geom_point(alpha = 0.25) +
  annotate(geom = "point", x = c(17863.06, 26138.85), y = c(99999, 5672), 
           size = 4) +
  # Create a nicer income scale
  scale_x_continuous(name = "Income", labels = scales::comma) +
  # Add horizontal line at $99 999
  geom_hline(yintercept = 99999, linetype = "dashed", colour = I("darkgrey")) +
  # Label individuals that work over 60 hours and report < $30 000 of income
  geom_label(data = nonzero_adj %>% filter(Hours > 60 & Income < 30000), 
             aes(label = paste0("Income = $", Income), 
                 size = 12), 
             x = c(37000, 37000), y = c(15000, 90000), 
             show.legend = FALSE, colour = "black") + 
  # Label individuals that have a risk adjustment > $99 999
  geom_label(data = nonzero_adj %>% filter(RISK_Adjustment > 99999), 
             aes(label = "Outlier, or possible miscode?", size = 12), 
             x = 110000, y = 112000, show.legend = FALSE) + 
  ggtitle("Relationship between RISK_Adjustment and Income for Productive Audits")
```

In the next visualization, we will explore the relationship between annual income and hourly wage. While we are not provided directly with each individual's hourly wage, we are given the average number of hours worked per week. We will make the assumption that each person works for 50 weeks per year. In the plot below, we group by audit status (productive is blue, nonproductive is pink) and identify large hourly wages.

```{r}
# Compute the "hourly wage" of individuals, assuming they work 50 weeks/year
audit_hourly_wage <- audit_edu %>% 
  mutate(Hourly_Wage = Income / ( 50 * Hours))

# Plot income vs. hourly wage, grouped by audit status
ggplot(audit_hourly_wage, 
       aes(x = Hourly_Wage, y = Income, colour = TARGET_Adjusted)) +
  geom_point() +
  # Add a vertical line for incomes above $1000
  geom_vline(xintercept = 1000, linetype = "dashed", colour = I("darkgrey")) +
  # Label points over an hourly wage of $1500
  geom_label(data = audit_hourly_wage %>% filter(Hourly_Wage > 1500), 
             aes(label = paste0("Hourly wage = $", Hourly_Wage)),
             x = 1900, y = 1e5, show.legend = FALSE) + 
  scale_x_continuous(name = "Hourly Wage", labels = scales::comma) +
  scale_y_continuous(name = "Income", labels = scales::comma) +
  ggtitle("Relationship between Income and Hourly Wage")
```

We see that there is a clear relationship between annual income and hourly wage, with a handful outliers. 

We now dig deeper into the hourly wages, grouping by occupation and audit status.

```{r}
# Visualize hourly wages, grouped by occupation and audit status
ggplot(audit_hourly_wage, aes(x = Occupation, y = Hourly_Wage,
                     colour = TARGET_Adjusted,
                     size = RISK_Adjustment)) +
  geom_point(alpha = 0.5) +
  # Add horizontal line at $1000
  geom_hline(yintercept = 1000, linetype = "dashed", color = I("darkgrey")) +
  # Label the x-axis vertically
  theme(axis.text.x = element_text(angle = 90)) +
  # Label points over an hourly wage $1500
  geom_label(data = audit_hourly_wage %>% filter(Hourly_Wage > 1500), 
             aes(label = paste0("Hourly wage = $", Hourly_Wage)),
             x = 11.75, y = 2500, show.legend = FALSE, size = 4) + 
  scale_y_continuous(name = "Hourly Wage", labels = scales::comma) +
  ggtitle("Hourly Wage, Grouped by Occupation and Audit Status")
```

We see that 2 of the largest hourly wages have missing values for their occupations. One observation that stands out as a possible error is the individual who makes $1372 per hour, but whose occupation is "clerical". Additionally, we see that some of the largest risk adjustments are for those who have low hourly wages.

# Unsupervised Learning 

## Dimension Reduction

### Principal Components Analysis (PCA)

To perform dimension reduction, we first perform principal components analysis. For this method, we can only use numeric variables. While we could convert categorical variables to numeric variables, it doesn't make sense to do so for PCA. This is because PCA relies on distances between vectors, and the distance between categorical variables is unclear.

We use the `prcomp` function, setting arguments `center` and `scale` to `TRUE` in order to normalize our data first.

The scree plot below shows the relative importance of each of the principal components by displaying the variance explained by each PC. 

```{r}
# Exclude all categorical variables, the response variable, and the ID variables.
audit_pca <- prcomp(audit[, c(2, 7, 9, 10)], center = TRUE, scale = TRUE)

plot(audit_pca, col = heat.colors(4),
     xlab = "Principal Component",
     main = "PCA on the Standardized Numerical Variables")
```

It is also useful to visualize the cumulative proportion of variation explained by each PC: 

```{r}
# How many principal components should we retain?
pr_var <- (audit_pca$sdev)^2

# Proportion of variance explained
pr_var_exp <- pr_var / sum(pr_var)

# Cumulative proportion of variance plot
barplot(cumsum(pr_var_exp), xlab = "PC #", ylab = "Proportion of explained variance", 
     main = "Cumulative proportion of variance plot", col = heat.colors(4),
     names.arg = 1:4)
```

The plot tells us that it would not be wise to remove any of our numeric variables (or to use a subset of the principal components), as removing even one PC will result in a drop of ~20% for the explained variance. Thus, we will need to explore other methods of dimension reduction (ones that will consider our categorical variables as well). 

In order to gain some insight from the PCs, we can create a "biplot", which will allow us to visualize the observations and direction vectors of multivariate data. In the plot below, the direction vectors which are longest in the x-direction correspond to the variables that are represented more heavily in the first PC, while direction vectors which are longest in the y-direction correspond to the variables that are represented more heavily in the second PC. 

```{r}
# Plot each variable vector components for the first 2 PCs
biplot(audit_pca, sub = "Biplot of first 2 principal components", scale = 0)
```

We can see from the biplot that the first principal component corresponds most to `Income` and `Age`. Similarly, the second principal component corresponds most to `Hours` and `Deductions`. We can confirm our results by looking at the rotation matrix: 

```{r}
# Load package for displaying tables
library(kableExtra)

# Display rotation matrix 
kable(audit_pca$rotation) %>% 
  kable_styling(latex_options = "striped")
```

### Self-organizing maps (SOM)

Self-organizing maps were used as another potential dimension-reduction technique. We employed a popular R package called `kohonen` to create the self-organizing maps. This algorithm was run on the four numeric variables in the dataset. These variables were standardized using a correlation transformation. The figure below shows a five-by-five "codes" plot. Individual cases are mapped to each circle (node) in the plot based on shared characteristics in the data. The relative size of each coloured slice in a node denotes the relative level of that variable compared to the others.  

```{r}
# Load package for creating SOMs 
library(kohonen)

# Define correlation transformation function (from section 3 of the lecture notes)
f.data.std <- function(data) {
  data <- as.matrix(data) 
  bar <- apply(data, 2, mean) 
  s <- apply(data, 2, sd) 
  t((t(data) - bar)/s)
}

# Create SOM and "codes" plot 
g <- kohonen::somgrid(xdim = 5, ydim = 5, topo = "rectangular")
X <- as.matrix(f.data.std(audit[, c(2, 7, 9:10)]))
map <- kohonen::som(X, grid = g)
plot(map,type = 'codes', main = " 5x5 codes plot") 
```

__Going to review some of the plots in the visualization section to just reiterate some of those patterns here.__

```{r}
# Plot the number of cases per node
plot(map, type = "count") 
```

This plot shows the counts for cases in each node of the codes plot. Many nodes contain under fifty cases, though some nodes appear to contain over 100 cases, and one node over 200 cases. 

## Clustering

### K-means clustering

K-means clustering was performed using the numeric variables in the audit dataset (i.e. `Age`, `Income`, `Deductions`, `Hours`). These variables were standardized using a correlation transformation (function from section three of the class notes). The algorithm was run to produce sets of two to fourteen clusters. The within cluster sum of squares and the Davies-Bouldin index were also calculated for each cluster to determine the appropriate number. We were hoping to perhaps see a two-cluster split of the data for productive and non-productive audits. The figure below shows a four-by-four array of plots: The first thirteen are scatter plots showing `Age` vs. `Deductions` -- two variables that appeared to show some clustering in our scatter plot matrices -- with the clusters highlighted in different colours; the last three include seperate plots for the within cluster sum of squares and the Davies-Bouldin index for different numbers of clusters, followed by a scatter plot of `Age` vs. `Deductions` with the `TARGET_Adjusted` variable coloured for comparison with the two-cluster plot.  

```{r}
# Load Dr. Mills' code 
source("vq_helpers.r")

# Set seed for reproducibility 
set.seed(123)

# Set up
my_colours <- sample(rainbow(14))
oldpar <- par(mfrow = c(4, 4))
par(mar = c(2, 1, 2, 1))
errs <- rep(0, 10)
DBI <- rep(0, 10)

# Peform kmeans clustering for varying values of k
for (i in 2:14) {
  KM <- kmeans(f.data.std(audit[, c(2, 7, 9:10)]), i, 50)

  if (i == 2) {
    cluster_labels <- KM$cluster

    # Calculate the proportion of correctly labelled points
    prop_correct_km <- round(sum(cluster_labels == as.numeric(audit$TARGET_Adjusted)) /  nrow(audit), 5)
    plot(x = audit$Age,y = audit$Deductions, col = my_colours[KM$cluster],
         main = paste(i," clusters"))
  } else {
    KM <- kmeans(f.data.std(audit[,c(2, 7, 9:10)]), i, 50)
    cluster_labels <- KM$cluster
    plot(x = audit$Age,y = audit$Deductions, col = my_colours[KM$cluster],
         main = paste(i," clusters"))
  }
  errs[i-1] <- sum(KM$withinss)
  DBI[i-1] <- Davies.Bouldin(KM$centers, KM$withinss, KM$size)
}

# Plot SS
plot(2:14, errs, type = "b", main = "SS")

# Plot Davies-Bouldin
plot(2:14, DBI, type = "b", main = "Davies-Bouldin")

# Plot with `TARGET_Adjusted` coloured in 
plot(x = audit$Age, y = audit$Deductions, type = "p", 
     xlim = c(10, 90), ylim = c(0, 3000),
     col = my_colours[audit$TARGET_Adjusted], main = "Plot for Target")

par(oldpar)

DBI.min <- which(DBI == min(DBI)) +1
err_diff <- diff(errs)
max_drop <- min(err_diff)
k_max_drop <- which(err_diff == max_drop) + 1
```

The Davies-Bouldin index shows that two clusters indeed appears to be the best number for these variables, although the largest drop in the within cluster sum of squares appeared between four and five clusters. The plot for `TARGET_Adjusted` matches fairly well with the two-cluster plot. The two-cluster plot appears to show all of the zero-deduction cases in one cluster, with a few of the lower decuction cases, and the higher deduction cases as a separate cluster. The `TARGET_Adjusted` plot shows that many of these higher-deduction cases were productive audits, as were a smaller number of the zero-deduction cases. Comparing the number of cases in the two clusters versus those for the `TARGET_Adjusted` variable, we calculated that approximately 77% of the cases were correctly clustered. 

### Hierarchical clustering

Hierarchical clustering was also used to see how how well the data appears to segregate into two clusters. Plots showing the results for six different linkage methods respectively are shown in the figure below, along with proportion of correctly clustered cases when compared with the `TARGET_Adjusted` variable. 

```{r}
audit_dist <- dist(audit[, c(2, 7, 9:10)], method = "euclidean")

# Try various linkage methods
hc_single <- hclust(audit_dist, method = "single")
hc_complete <- hclust(audit_dist, method = "complete")
hc_average <- hclust(audit_dist, method = "average")
hc_ward <- hclust(audit_dist, method = "ward.D")
hc_ward2 <- hclust(audit_dist, method = "ward.D2")
hc_mcquitty <- hclust(audit_dist, method = "mcquitty")

# Plot dendrograms, add proportion of correctly labelled points underneath
par(mfrow = c(3, 2))

# Single Linkage
plot(hc_single, cex = 0.75, labels = FALSE) 
rect.hclust(hc_single, k = 2)
cluster_labels <- cutree(hc_single, k = 2)
prop_correct_single <- round(sum(cluster_labels == as.numeric(audit$TARGET_Adjusted)) / nrow(audit), 5)
correct_single <- paste("Proportion correct:", prop_correct_single) 
mtext(text = correct_single, side = 1, line = 1)

# Complete Linkage
plot(hc_complete, cex = 0.75, labels = FALSE) 
rect.hclust(hc_complete, k = 2)
cluster_labels <- cutree(hc_complete, k = 2)
prop_correct_complete <- round(sum(cluster_labels == as.numeric(audit$TARGET_Adjusted)) / nrow(audit), 5)
correct_complete <- paste("Proportion correct:", prop_correct_complete) 
mtext(text = correct_complete, side = 1, line = 1)

# Average Linkage
plot(hc_average, cex = 0.75, labels = FALSE) 
rect.hclust(hc_average, k = 2)
cluster_labels <- cutree(hc_average, k = 2)
prop_correct_average <- round(sum(cluster_labels == as.numeric(audit$TARGET_Adjusted)) / nrow(audit), 5)
correct_average <- paste("Proportion correct:", prop_correct_average) 
mtext(text = correct_average, side = 1, line = 1)

# Ward Linkage
plot(hc_ward, cex = 0.75, labels = FALSE) 
rect.hclust(hc_ward, k = 2)
cluster_labels <- cutree(hc_ward, k = 2)
prop_correct_ward <- round(sum(cluster_labels == as.numeric(audit$TARGET_Adjusted)) / nrow(audit), 5)
correct_ward <- paste("Proportion correct:", prop_correct_ward) 
mtext(text = correct_ward, side = 1, line = 1)

# Ward-2 Linkage
plot(hc_ward2, cex = 0.75, labels = FALSE) 
rect.hclust(hc_ward2, k = 2)
cluster_labels <- cutree(hc_ward2, k = 2)
prop_correct_ward2 <- round(sum(cluster_labels == as.numeric(audit$TARGET_Adjusted)) / nrow(audit), 5)
correct_ward2 <- paste("Proportion correct:", prop_correct_ward2) 
mtext(text = correct_ward2, side = 1, line = 1)

# McQuitty Linkage
plot(hc_mcquitty, cex = 0.75, labels = FALSE) 
rect.hclust(hc_mcquitty, k = 2)
cluster_labels <- cutree(hc_mcquitty, k = 2)
prop_correct_mcquitty <- round(sum(cluster_labels == as.numeric(audit$TARGET_Adjusted)) / nrow(audit), 5)
correct_mcquitty <- paste("Proportion correct:", prop_correct_mcquitty) 
mtext(text = correct_mcquitty, side = 1, line = 1)
```

While the single-, average-, and McQuitty-linkage methods appear to give higher levels of proportion correct (>0.76 each), one can see that virtually all of the cases have been allocated to a single cluster. Since approximately 23% of the cases are productive audits, it is clear that this clustering isn't really separating out any of the productive audits. Overall, the hierarchical clustering does not appear to do a reasonably good job for any of the six linakge methods. 

# Supervised Learning

## Set-Up: Training and Test Set

Before we begin supervised learning, we should split our data into a training and test set so that we can evaluate the performance of our models. We will also re-normalize our numeric predictor variables, within the training set and testing set separately. In the real world, if we only had a training set, we would not have the test set to inform our normalization, so it is essential to normalize them separately if we want to accurately measure the performance of our models. 

Once the training and test sets have been created, it is important to examine the distribution of the classes (and the response variable) between the training and test set. Since there are very few productive audits in our dataset, we need to ensure that both productive and nonproductive audits are represented adequately in both the training set and the test set. The following plots show the distributions of the response variables of interest, using an 80 / 20 split for training / test size.

```{r}
# Set seed for reproducibility
set.seed(12)

# Create test training split 
n <- nrow(audit)
ntrain <- round(0.8 * n, 0)
ntest <- nrow(audit) - ntrain 
train_indices <- sample(1:n, size = ntrain, replace = FALSE)
train_audit <- audit[train_indices, ]
test_audit <- audit[-train_indices, ]

# Normalization function
f.data.std <- function(data) {
data <- as.matrix(data) 
bar <- apply(data, 2, mean) 
s <- apply(data, 2, sd) 
t((t(data) - bar)/s)
}

# Normalize numeric predictor variables, separately between training and test sets
train_audit$Age <- f.data.std(train_audit$Age)
train_audit$Income <- f.data.std(train_audit$Income)
train_audit$Deductions <- f.data.std(train_audit$Deductions)
train_audit$Hours <- f.data.std(train_audit$Hours)
test_audit$Age <- f.data.std(test_audit$Age)
test_audit$Income <- f.data.std(test_audit$Income)
test_audit$Deductions <- f.data.std(test_audit$Deductions)
test_audit$Hours <- f.data.std(test_audit$Hours)

# Create data frame to be used for modeling the RISK_Adjustment variable
# Remove ID variable, and TARGET_Adjusted variable
train_audit_m <- train_audit[, -c(1, 12)]
test_audit_m <- test_audit[, -c(1, 12)]

# Create data frame to be used for classifying the TARGET_Adjusted variable
# Remove the ID variable, and RISK_Adjustment variable
train_audit_c <- train_audit[, -c(1, 11)]
test_audit_c <- test_audit[, -c(1, 11)]

# Visualize the distribution of classes (and response) between training / test sets
par(mfrow = c(2, 2))
barplot(prop.table(table(train_audit$TARGET_Adjusted)), main = "Training set",
        xlab = "TARGET_Adjusted", ylab = "Proportion of observations")
barplot(prop.table(table(test_audit$TARGET_Adjusted)), main = "Test set", 
        xlab = "TARGET_Adjusted", ylab = "Proportion of observations")
hist(train_audit$RISK_Adjustment, probability = TRUE, main = "Training set", 
     xlab = "RISK_Adjustment", ylab = "Proportion of observations",
     xlim = c(0, 120000), col = "grey")
hist(test_audit$RISK_Adjustment, probability = TRUE, main = "Test set", 
     xlab = "RISK_Adjustment", ylab = "Proportion of observations",
     xlim = c(0, 120000), col = "grey")
```

The distributions look approximately equal, and we can proceed.

## Classification 

In this section, we will use a variety of methods to predict the binary variable `TARGET_Adjusted` for the test set, using the training set to develop and validate models. 

### Generalized Linear Models / Logistic Regression

We begin our classification with logistic regression. We will start with a very simple model that uses `Gender` to predict audit status, noting that a higher proportion of males resulted in productive audits for our training set: 

```{r}
# Examine the proportion of males vs. females that resulted in productive audits
prop.table(table(train_audit_c$TARGET_Adjusted, train_audit_c$Gender))

# Create GLM to predict TARGET_Adjusted using Gender as the predictor
glmod1 <- glm(TARGET_Adjusted ~ Gender, data = train_audit_c, family = "binomial")

# Calculate prediction accuracy on the test set 
glmod1_predict <- round(predict(glmod1, newdata = test_audit_c, type = "response"), 0)
prop_correct_glmod1 <- sum(glmod1_predict == test_audit_c$TARGET_Adjusted) / ntest
```

This simple model classified every single observation in the test set as a 0 (nonproductive audit), with the largest probability of productive audit being assigned 0.28. Since there are only 93 productive audits in the test set of 400 observations, it's not surprising that the classification accuracy was 307/400 = 76.75%. 

Below, we plot the predicted probabilities of being a productive audit (for the training data). The dashed lines represent the null probabilities (using the table of proportions from the training set, shown earlier). 

```{r}
# Create data frame containing the predicted probabilities and Gender
predicted_data <- data.frame(
  probability_of_prod = glmod1$fitted.values,
  Gender = train_audit_c$Gender)
 
# Plot the predictions 
ggplot(data = predicted_data, aes(x = Gender, y = probability_of_prod)) +
  geom_point(aes(color = Gender), size = 5) +
  xlab("Gender") +
  ylab("Predicted probability of being a productive audit") +
  geom_hline(yintercept = 313 / (313 + 798), linetype = "dashed", color = "#00BFC4") +
  geom_hline(yintercept = 57 / (57 + 432), linetype = "dashed", color = "#F8766D")
```

Therefore, the model is simply predicting the conditional probabilities from the training data.

Next, we will fit the full model (using all available predictor variables). Note that in order to do this, we will need to deal with the missing values. Since our missing values are only for the categorical variables and since we did not discuss in class how to interpolate categorical variables, we will simply remove these observations when training any models which cannot handle missing values. 

```{r}
# Keep only the observations without missing values
complete_train_audit_c <- train_audit_c[complete.cases(train_audit_c), ]
complete_test_audit_c <- test_audit_c[complete.cases(test_audit_c), ]
complete_train_audit_m <- train_audit_m[complete.cases(train_audit_m), ]
complete_test_audit_m <- test_audit_m[complete.cases(test_audit_m), ]

# Compute the number of observations that had no missing values
ntrain_complete <- nrow(complete_train_audit_c)
ntest_complete <- nrow(complete_test_audit_c)

# Create GLM using all predictor variables
glmod2 <- glm(TARGET_Adjusted ~ ., data = complete_train_audit_c, family = "binomial")

# Calculate prediction accuracy on the test set 
glmod2_predict <- round(predict(glmod2, newdata = complete_test_audit_c, type = "response"), 0)
prop_correct_glmod2 <- sum(glmod2_predict == complete_test_audit_c$TARGET_Adjusted) / ntest_complete
```

The accuracy of the full model is 87.17%. This is a much higher accuracy than the model which only used `Gender`. We can visualize the predictions on the training set using the model computed with all predictor variables, grouping by `Gender` again for comparison to the previous model:  

```{r}
# Create data frame containing the predicted probabilities and Gender
predicted_data <- data.frame(
  probability_of_prod = glmod2$fitted.values,
  Gender = complete_train_audit_c$Gender)
 
# Plot the predictions 
ggplot(data = predicted_data, aes(x = Gender, y = probability_of_prod)) +
  geom_boxplot(aes(color = Gender)) +
  xlab("Gender") +
  ylab("Predicted probability of being a productive audit") +
  geom_hline(yintercept = 313 / (313 + 798), linetype = "dashed", color = "#00BFC4") +
  geom_hline(yintercept = 57 / (57 + 432), linetype = "dashed", color = "#F8766D")
```

Now, there is a much wider range of values that the model will predict.

We have now created a very simplistic model (one predictor), and a very complex model (all predictors). 

### k-NN (k Nearest Neighbours)

Another simple classification method is *k Nearest Neighbours.* In this method, the $k$ closest vectors in the training set (in terms of Euclidean distance) are found for each observation in the test set. The classification of an observation in the test set is then decided by performing a "vote" using the $k$ closest vectors. If there are any ties, the ties are broken at random. Additionally, if there are ties in terms of finding the $k^{th}$ nearest vector, all of the candidate vectors are included in the "voting" process. We will be using the `class` package to perform the classification.

Since we are dealing with the Euclidean distance, we will have to consider only our numeric variables in this case.

In the figure below, we plot the proportion of correctly classified observations in the test set as a function of the value of $k$. 

```{r}
# Load library for performing kNN
library(class)

# Set seed for reproducibility 
set.seed(19)

# Get just the numeric variables 
audit_num_train <- train_audit_c[, c(1, 6, 8:9), ]
audit_num_test <- test_audit_c[, c(1, 6, 8:9), ]

# Try varying values of k 
k <- 1:50

# Perform kNN 
knn_test_acc <- vector(length = 50)
for (i in 1:length(knn_test_acc)) {
  knn_audit <- knn(train = audit_num_train, test = audit_num_test, 
                   cl = train_audit_c$TARGET_Adjusted, k = k[i])
  knn_test_acc[i] <- sum(knn_audit == test_audit_c$TARGET_Adjusted) / ntest
}

# Create plot of results
plot(k, knn_test_acc, main = "Test accuracy of k Nearest Neighbours", 
     xlab = "k", ylab = "Test set accuracy", type = "l")
```

From the plot, we can see that the value of $k$ which resulted in the highest classification accuracy on the test set was $k = 22$, resulting in an accuracy of 79%.

### Linear and Quadratic Discriminant Analysis

*Discriminant analysis* is a classification technique that uses the training data to determine the boundaries which best separate the classes. We treat the observations of each class as observations from a multivariate normal (MVN) distribution, and compute the boundaries using a formula which depends on the parameters of the MVN distributions. If we assume that the covariance matrices of all the MVN distributions are the same, we perform *Linear Discriminant Analysis* (LDA) and the resulting boundaries are lines. If we relax this assumption, we perform *Quadratic Discriminant Analysis* (QDA) and the resulting boundaries are quadratic. LDA is computationally easy to perform, but linear boundaries may not always be appropriate. QDA gives us more freedom with our boundaries, at the expense of taking longer to perform. 

We will use the `MASS` package to perform the discriminant analysis. 

```{r}
# Load library
library(MASS)

# Perform LDA 
audit_lda <- lda(formula = TARGET_Adjusted ~ ., 
                 data = complete_train_audit_c[, c(1, 6, 8:10)])
lda_predict <- predict(audit_lda, complete_test_audit_c)
lda_test_acc <- sum(lda_predict$class == complete_test_audit_c$TARGET_Adjusted) / ntest_complete

# Perform QDA
audit_qda <- qda(formula = TARGET_Adjusted ~ ., 
                 data = complete_train_audit_c[, c(1, 6, 8:10)])
qda_predict <- predict(audit_qda, complete_test_audit_c)
qda_test_acc <- sum(qda_predict$class == complete_test_audit_c$TARGET_Adjusted) / ntest_complete
```

With LDA, our proportion of correctly classified test observations is 0.79, while it is 0.785 with QDA. Although this may make it seem like LDA is the superior method, this is not the case -- LDA is a subset of QDA, and thus QDA *should* be able to obtain the same results as LDA given a large enough training set that is representative of the testing set. 

Since we performed the discriminatory analysis on more than two numeric variables, we are unable to visualize the results the exact way we did in lecture. There will not be clear lines (or quadratic curves) that separate the class predictions *perfectly* on a 2-dimensional plot, since the calculations were completed in 4 dimensions. Instead, we can create plots of the different 2 variable combinations, using the `klaR` package. Note that the plots below are created for the training data, and do not include any information about the test data.

```{r}
# Load library 
library(klaR)

# Note: The partimat function calls the lda() and qda() functions from the MASS package
# Create plot of LDA results on each of the 2 variable combinations 
partimat(formula = TARGET_Adjusted ~ ., data = complete_train_audit_c[, c(1, 6, 8:10)], 
         method = "lda", main = "LDA on Audit data")

# Create plot of QDA results on each of the 2 variable combinations 
partimat(formula = TARGET_Adjusted ~ ., data = complete_train_audit_c[, c(1, 6, 8:10)], 
         method = "qda", main = "QDA on Audit data")
```

### Classification trees

Another way of classifying our observations into productive and nonproductive audits is through the use of classification trees. There are many ways in R to create classification trees, but we will start by using the `tree` package. The figure below shows the resulting classification tree on the training dataset, using the deviance as the measure. Note that the generalized gini index plot has been excluded, as it is unreadable (highly concentrated at the bottom of the tree, implying that the decrease in impurity is fairly low in the bottom branches -- pruning may be necessary). 

```{r, message = FALSE}
# Load libraries for creating decision trees
library(tree)

# Use deviance:
# Note: mincut is the minimum number of observations per node
tree_dev <- tree(formula = TARGET_Adjusted ~., data = train_audit_c,
                 split = "deviance", mincut = 5)
# Use generalized gini
tree_gini <- tree(formula = TARGET_Adjusted ~., data = train_audit_c,
                  split = "gini", mincut = 5)

# Load Dr. Mills' code
source("ejnplottree.r")
source("ejntexttree.r")
source("EJNPartitionTree.r")

# Visualize the results 
plot.tree.EJN(tree_dev)
text.tree.EJN(tree_dev, all = TRUE)
# plot.tree.EJN(tree_gini)

# Calculate prediction accuracy on the test set 
tree_dev_predict <- predict(tree_dev, test_audit_c, type = "class")
tree_gini_predict <- predict(tree_gini, test_audit_c, type = "class")
prop_correct_dev <- length(which(tree_dev_predict == test_audit_c$TARGET_Adjusted)) / ntest
prop_correct_gini <- length(which(tree_gini_predict == test_audit_c$TARGET_Adjusted)) / ntest
```

Using the proportion of correctly classified observations in the test set as a measure of model accuracy, the above models had accuracies on the test set of 86.5% and 82.5% respectively.

One problem with the plotting that we can see immediately is that the `tree` package does not plot categorical variables very well. Rather than displaying the names of categories, the plot displays a concatenation of letters corresponding to the positions of the level names. For example, the first split on the tree on the left is based on the `Marital` variable. The plot includes the letters "abdef" on this split, which actually means levels 1, 2, 4, 5 and 6 of the `Marital` variable (corresponding to the categories "Absent", "Divorced", "Married-spouse-absent", "Unmarried" and "Widowed"). 

In order to create a plot that is a bit more clear about the decision rules, we can use the `rpart` and `rpart.plot` packages. We note that the trees may appear slightly different between the `tree` package implementation and the `rpart` package implementation even if the trees are fairly similar (the `rpart` package does not always create decision rules in the form *var < split*). The plot below shows the classification tree created by the `rpart` package, using the entire dataset. We have used the same value for the minimum number of observations per node as we used in the previous two plots: 

```{r}
# Load libraries for creating more visually appealing classification trees
library(rpart)
library(rattle)
library(RColorBrewer)
library(rpart.plot)

# Create decision tree
# Note: minsplit is the minimum number of observations per node
tree_rpart <- rpart(formula = TARGET_Adjusted ~., data = train_audit_c,
                    method = "class", minsplit = 5)

# Plot the tree
fancyRpartPlot(tree_rpart, sub = "Classification tree created using the rpart package")
```

Here, we can see that the plot becomes much more readable. For example, the first node now shows us which levels of the `Marital` variable correspond with going left or right down the tree. 

#### Pruning 

*Pruning* classification trees can help to prevent overfitting, removing the least important splits from our tree. The `cv.tree` function will perform K-fold cross validation for us, determining the best splits to prune. 

The plot below shows the size of the tree versus the deviance (using 10-fold cross validation) for the two trees created using the `tree` package, and we can see that the lowest deviance occurs when the size of the tree is 4 for the first tree, and when the size is 5 for the second tree. 

```{r}
# Perform 10-fold CV 
tree_cv_dev <- cv.tree(tree_dev, FUN = prune.tree, K = 10)
tree_cv_gini <- cv.tree(tree_gini, FUN = prune.tree, K = 10)

# Plot the size of the tree versus the deviance
par(mfrow = c(1, 2))
plot(tree_cv_dev)
plot(tree_cv_gini)
```

We can now use the results of this cross validation to prune the trees to the suggested sizes. The accuracy on the test set has now increased: 

```{r}
# Use the results of the cross validation to prune the trees
pruned_tree_dev <- prune.misclass(tree_dev, best = 4)
pruned_tree_gini <- prune.misclass(tree_gini, best = 5)

# Compute classification accuracy on the test set
pruned_tree_dev_pred <- predict(pruned_tree_dev, test_audit_c, type = "class")
pruned_tree_gini_pred <- predict(pruned_tree_gini, test_audit_c, type = "class")
prop_correct_dev_prune <- length(which(pruned_tree_dev_pred == test_audit_c$TARGET_Adjusted)) / ntest
prop_correct_gini_prune <- length(which(pruned_tree_gini_pred == test_audit_c$TARGET_Adjusted)) / ntest

# Create table to display the results
results <- data.frame(Original = c(prop_correct_dev, prop_correct_gini),
                      Pruned = c(prop_correct_dev_prune, prop_correct_gini_prune))
row.names(results) <- c("Deviance", "Generalized Gini")

# Display the results
library(kableExtra)
kable(results) %>% 
  kable_styling(latex_options = "striped")
```

We can see that the pruning had a positive impact on the second tree, reducing overfitting and improving the test accuracy. 

#### Bagging 

*Bagging*, or Boostrap Aggregating, is a method we can use to improve the stability of algorithms (such as classification trees). 

Within our training data, we will do the following a total of 100 times: 

- Split the data into a train and test set; let the size of the train set be $N$
- Create 50 bootstrap replicates (creating a sample of size $N$ by sampling from the training set with replacement)
- Create a tree for each of the 50 bootstrap replicates, pruning as needed
- Use the 50 trees to "vote" on the correct classes for each observation in the test set

At the end, the 100 results will be used to perform another vote. 

```{r}
# Set seed for reproducibility 
set.seed(2020)

# Create a function to get the most commonly occurring value in a vector
# We will use this to determine the "winner" of "votes"
# Do not consider 0s as this will give us problems when combining the 100 votes at the end
getmode <- function(x) {
   uniqx <- unique(x)
   if (0 %in% uniqx == FALSE) {
     return(uniqx[which.max(tabulate(match(x, uniqx)))])
   } else {
     x <- x[x != 0]
     uniqx <- unique(x)
   }
   return(uniqx[which.max(tabulate(match(x, uniqx)))])
}
```

```{r, eval = FALSE}
# Size of our dataset 
N <- round(0.9 * ntrain, 0)
# Number of bootstrap replicates we will take at each iteration
sz <- 50
# Create a matrix to store our results
bag <- matrix(0, nrow = ntrain, ncol = 100)
# Create a list to store all of our trees
rep.trees <- vector(mode = "list", length = 100)

# Perform the bagging 
for (j in 1:100) {
  # Create a train / test split (use 90 / 10)
  temp <- 1:ntrain 
  train_idx <- sample(temp, size = N
                      , replace = FALSE)
  test_idx <- temp[temp %in% train_idx == FALSE]
  train <- train_audit_c[train_idx, ]
  test <- train_audit_c[test_idx, ]
  size_train <- N
  size_test <- ntrain - size_train
  
  # Create the bootstrap replicates and vote 
  replicates <- matrix(0, nrow = sz, ncol = N)
  rep.trees[[j]] <- vector(mode = "list", length = sz)
  pred.tree <- matrix(0, nrow = size_test, ncol = sz)
  for (i in 1:sz) {
    # Create 50 bootstrap replicates by sampling (w replacement) from original training set
    replicates[i, ] <- sample(1:N, size = N, replace = TRUE)
    
    # Create trees for each of the 50 bootstrap replicates
    rep.trees[[j]][[i]] <- rpart(formula = TARGET_Adjusted ~ ., 
                                 data = train[replicates[i, ], ])
    
    # Use the 50 trees to predict the classes of the test observations
    pred.tree[, i] <- predict(rep.trees[[j]][[i]], test, type = "class")
  }

  # Vote on the classes of the test observations, using the predictions from the 50 trees
  bag[test_idx, j] <- apply(pred.tree, 1, getmode)
}

# Save results in a file 
save(bag, file = "bag.RDa")
save(rep.trees, file = "rep.trees.RDa")
```

```{r}
# Load the saved files
load("bag.RDa")
load("rep.trees.RDa")

# Vote on the classes of the observations, using the 100 iterations 
bag.class <- apply(bag, 1, getmode)

# Check the training accuracy 
train_acc_bag <- sum(bag.class == as.numeric(train_audit_c$TARGET_Adjusted)) / ntrain 

# Use the models to predict the classes of the test set
bag_test <- matrix(0, nrow = ntest, ncol = 100)
for (j in 1:100) {
  temp_predictions <- matrix(NA, nrow = ntest, ncol = 50)
  for (i in 1:50) {
    temp_predictions[, i] <- as.numeric(predict(rep.trees[[j]][[i]], 
                                                      test_audit_c, type = "class"))
  }
  bag_test[, j] <- apply(temp_predictions, 1, FUN = getmode)
}
# Vote on the classes of observations in test_audit_c, using the 100 iterations
bag_test_class <- apply(bag_test, 1, FUN = getmode)

# Check the prediction accuracy on the test set
test_acc_bag <- sum(bag_test_class == as.numeric(test_audit_c$TARGET_Adjusted)) / ntest
```

Using the bagging, we obtain a training accuracy (on our 1600 observations) of 83.0625%. Using the models from the bagging on our reserved test set (400 observations), we obtain a test accuracy of 88.25%, which is a huge improvement! The bagging was highly successful.

#### Random Forests

*Random Forests* are an extension of bagging, where features are also randomly selected. Although we did not cover this topic during lecture, random forests are a very popular technique and are easy to create in R, so we will try a simple model. 

Note that with the `randomForest` function in R, we need to choose a way of handling missing data. Here, we will simply omit the incomplete observations. 

When using the `randomForest` function in R, there is one main parameter to tune: `mtry`. This is the number of variables that are randomly sampled as candidates at each split. The plot below shows the model accuracy on the test set for varying values of `mtry`: 

```{r}
# Load library for creating random forests
library(randomForest)

# Set seed for reproducibility 
set.seed(41)

# Since we have 9 predictor variables, we will vary the value of mtry from 1 to 9
mtry <- 1:9

# Create the random forests, store the accuracy on the test set 
rf_test_acc <- vector(length = 9)
for (i in 1:9) {
  rf <- randomForest(formula = TARGET_Adjusted ~., data = train_audit_c, 
                     na.action = na.omit, mtry = mtry[i], ntree = 500)

  # Predict the classes of the test set using the random forest model 
  rf_predict <- predict(rf, newdata = test_audit_c)

  # Compute the accuracy on the test set 
  matches <- rf_predict == test_audit_c$TARGET_Adjusted
  rf_test_acc[i] <- sum(matches, na.rm = TRUE) / length(which(is.na(matches) == FALSE)) 
}

plot(mtry, rf_test_acc, type = "l", main = "Test Accuracy using Random Forests", 
     xlab = "Value of mtry", ylab = "Test accuracy (percent)")
```

The highest accuracy occurred when `mtry` was set to 4. In this case, the prediction accuracy on the test set was 87.2%. While this is a bit lower than what we obtained with bagging, it should be noted that the computational time for this method was significantly lower than it was for bagging. 

### Neural Networks

Finally, we will use neural networks to classify our observations into productive and nonproductive audits. We begin by using the `nnet` package, starting with 0 hidden layers and a weight decay parameter of 0.0005. Since the results will depend on the (randomly chosen) initial weights, we will try 100 different initial weighting options. We note that our categorical variables are converted to one-hot encoded variables, and thus we end up with a total of 44 weights for each initialization. We can't *really* visualize this, so we instead plot the iteration number vs. the minima reached, and the iteration number vs. the classification accuracy on the test data.

```{r, eval = FALSE}
# Load package for creating neural networks
library(nnet)

# Set seed for reproducibility 
set.seed(17)

# Use the nnet package to create neural networks with 0 hidden layers 
nnet_results <- matrix(nrow = 100, ncol = 2)
for (i in 1:100) {
  audit_nnet <- nnet(formula = TARGET_Adjusted ~ ., data = complete_train_audit_c, 
                     size = 0, skip = TRUE, range = 0.1, decay = 5e-4, maxit = 500)
  nnet_predict <- round(predict(audit_nnet, complete_test_audit_c), 0)
  nnet_results[i, 1] <- audit_nnet$value
  nnet_results[i, 2] <- sum(nnet_predict[, 1] == complete_test_audit_c$TARGET_Adjusted) / ntest_complete
}

# Save results
save(nnet_results, file = "nnet_results.RDa")
```

```{r}
# Load results 
load("nnet_results.RDa")

# Plot results from the nnet function
par(mfrow = c(1, 2))
plot(nnet_results[, 1], xlab = "Iteration Number", ylab = "Minima",
     main = "Results from nnet function")
plot(nnet_results[, 2], xlab = "Iteration Number", 
     ylab = "Classification Accuracy on Test Set",
     main = "Results from nnet function", 
     ylim = c(0.871, 0.872))
```

We see that, in this case, the different initial weightings *did not* have an effect on our classification accuracy -- in every case, the accuracy was 87.17%. The minima found "jumped around" a bit, but was the same value within 3 decimal points regardless of the starting points. 

This accuracy was very high even with such a simple model, but we will now try to improve it. We will use the `ANN2` package, which will allow us to tune more parameters (such as activation functions, optimization types), and apply regularization techniques that we learned about in our Deep Learning research project.

```{r, eval = FALSE}
# Load package for creating neural networks 
library(ANN2)

# Set seed for reproducibility
set.seed(29)

# Convert categorical variables to one-hot encoded variables
one_hot_train <- model.matrix(object = TARGET_Adjusted ~ . - 1, 
                                data = complete_train_audit_c)
one_hot_test <- model.matrix(object = TARGET_Adjusted ~ . - 1, 
                             data = complete_test_audit_c)

# Create a parameter tuning "grid" 
parameter_grid <- expand.grid(activ.functions = c("tanh", "sigmoid", "relu", "linear"),
                              optim.type = c("sgd", "rmsprop", "adam"),
                              L2 = seq(0, 1, by = 0.1))

nn_test_acc <- vector(length = nrow(parameter_grid))
for (i in 1:nrow(parameter_grid)) {
  # Create neural network
  audit_nn <- neuralnetwork(X = one_hot_train, 
                            y = complete_train_audit_c[, 10], 
                            hidden.layers = NA, 
                            loss.type = "log", 
                            regression = FALSE,
                            activ.functions = as.character(parameter_grid$activ.functions[i]),
                            optim.type = as.character(parameter_grid$optim.type[i]), 
                            L2 = parameter_grid$L2[i])

  # Compute classification accuracy on the test set
  nn_predict <- as.numeric(predict(audit_nn, newdata = one_hot_test)$predictions)
  nn_test_acc[i] <- sum(nn_predict == complete_test_audit_c$TARGET_Adjusted) / ntest_complete
}
# Save results
save(nn_test_acc, file = "nn_test_acc.RDa")

# Find parameters for the best model
best_params <- parameter_grid[which.max(nn_test_acc), ]

# Set seed for reproducibility 
set.seed(94)

# Play around with more neurons 
num_neurons <- seq(2, 64, by = 1)
nn_test_acc_2 <- vector(length = length(num_neurons))
for (i in 1:length(num_neurons)) {
  # Create neural network
  audit_nn <- neuralnetwork(X = one_hot_train, 
                            y = complete_train_audit_c[, 10], 
                            hidden.layers = num_neurons[i], 
                            loss.type = "log", 
                            regression = FALSE,
                            activ.functions = as.character(best_params$activ.functions),
                            optim.type = as.character(best_params$optim.type), 
                            L2 = best_params$L2)

  # Compute classification accuracy on the test set
  nn_predict <- as.numeric(predict(audit_nn, newdata = one_hot_test)$predictions)
  nn_test_acc_2[i] <- sum(nn_predict == complete_test_audit_c$TARGET_Adjusted) / ntest_complete
}

# Save results
save(nn_test_acc_2, file = "nn_test_acc_2.RDa")
```

The activation functions we will search over are hyperbolic tangent, sigmoid, ReLU, and linear. We will also try three different optimization techniques: stochastic gradient descent, RMSProp (Hinton et al., 2012), and Adam (Kingma and Ba, 2014). Finally, we try varying values for $\lambda$ with L2 regularization. The plot below (left) shows the classification accuracy on the test data for the various combinations of parameters. The best accuracy was 88.22%, just 0.03% worse than our results from bagging classification trees. This was achieved using a linear activation function with stochastic gradient descent, and no hidden layers or regularization. 

Next, we add 1 hidden layer and vary the number of neurons in this hidden layer, from 2 to 64. The plot below (right) shows the classification accuracy on the test data as a function of the number of hidden neurons. The optimal choice was 39 neurons in the hidden layer, resulting in an accuracy of 88.48% on the test set, which was the highest accuracy we achieved with any of the models we tried. The computational time for the neural networks was also lower than for the bagging of classification trees (second best results).

```{r}
# Load results
load("nn_test_acc.RDa")
load("nn_test_acc_2.RDa")

# Plot the classification accuracy on the test set
par(mfrow = c(1, 2))
boxplot(nn_test_acc, type = "l", main = "Varying Neural Network Parameters", 
     ylab = "Classification Accuracy on Test Set (%)", 
     cex.main = 0.75)
plot(x = seq(2, 64, by = 1), y = nn_test_acc_2, type = "l",
     main = "Varying Number of Neurons in Hidden Layer", 
     xlab = "Number of neurons", 
     ylab = "Classification Accuracy on Test Set (%)",
     cex.main = 0.75)
```

## Modeling

In this section, we will use a variety of methods to predict the continuous response variable `RISK_Adjustment` for the test set, using the training set to develop and validate models. 

### Ordinary Least Squares Regression

To begin our modeling, we will use ordinary least squares regression (OLS). We will create all possible subsets of the predictors and find the *best* model, in terms of the SSE. The `leaps` package has a function which performs all possible subsets, and returns an object containing the best models for each number of predictors, along with performance metrics. 

The plot below shows the various performance criteria (on the training data) for the "best" model for each number of predictors.

```{r}
# Load package for performing all subsets regression 
library(leaps)

# Perform all subsets regression
lm_subsets <- regsubsets(x = RISK_Adjustment ~ .,
                         data = complete_train_audit_m,
                         method = "exhaustive", nvmax = 9)
sub_summary <- summary(lm_subsets)

# Plot the performance criteria
par(mfrow = c(2, 2))
plot(1:9, sub_summary$rss, xlab = "Number of predictors", ylab = "SSE", 
     type = "b", main = "SSE")
plot(1:9, sub_summary$bic, xlab = "Number of predictors", ylab = "BIC", 
     type = "b", main = "BIC")
plot(1:9, sub_summary$adjr2, xlab = "Number of predictors", ylab = expression("R^2"), 
     type = "b", main = "Adjusted R^2")
plot(1:9, sub_summary$cp, xlab = "Number of predictors", ylab = "C_p", 
     type = "b", main = "Mallow's C_p")
lines(1:9, 1:9, col = "red")
```

Mallow's $C_p$ is approximately equal to $p$ (red line) when there are 4 predictors. The BIC also happens to reach a minimum at $p = 4$, and the SSE and $R^2_{adj}$ see little improvement past this value.

```{r, eval = FALSE}
# Find which 4 predictors to include in the model
which(sub_summary$which[4, ] == TRUE)
```

By our selection criteria, the best model includes `Employment`, `Education`, `Marital`, and `Occupation` as the predictors.

```{r}
# Create the "best" model, as selected by our criteria
lmod <- lm(RISK_Adjustment ~ Employment + Education + Marital + Occupation,
           data = complete_train_audit_m)

# Predict on test data: 
lmod_predict <- predict(lmod, complete_test_audit_m)

# Get the SSE 
lmod_sse <- sum((complete_test_audit_m$RISK_Adjustment - lmod_predict)^2)

# Get the mean absolute error 
lmod_mae <- mean(abs(complete_test_audit_m$RISK_Adjustment - lmod_predict))

# Look at some diagnostic plots
# Residuals & Q-Q plot
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(lmod, which = c(1, 2))  # Plot the model information
```

Using our model to predict the response for the test set, we obtain an SSE of roughly 29.5 billion. The average error ends up being of roughly $3025. The poor performance of our model can be explained by the assumptions of linear regression not being satisfied. The residual plot indicates that the homoscedasticity assumption is not satisfied since our errors do not have constant variance. Furthermore, the Q-Q plot above indicates that the normality assumption is not satisfied.

### Multivariate Adaptive Regression Splines (MARS)

*Multivariate Adaptive Regression Splines*, commonly known as MARS, is a non-parametric way of modelling data using a series of piecewise linear regression models. 

While the `mda` package in R offers a `mars` function to perform MARS, there is a package called `earth` which uses cross validation to find the optimal placements of knots across all provided predictor variables, and prunes accordingly. The function continues searching until the change in $R^2$ is less than a specified threshold. 

This package also works well the `caret` package for further tuning of parameters. The main parameters that we wish to tune when using MARS are `degree` and `nprune`. The `degree` parameter controls the maximum degree of interaction terms that are allowed in the model, and the `nprune` parameter controls the maximum number of predictors that are allowed in the model. We will use the `caret` package to perform 10-fold cross validation to pick the best values for `degree` and `nprune`. The `caret` package recommends using RMSE (root mean squared error) as the selection criterion for parameter tuning when the response variable is continuous.  

```{r, eval = FALSE}
# Load packages for performing MARS and for tuning parameters
library(earth)
library(caret)

# Create a parameter tuning "grid" 
parameter_grid <- floor(expand.grid(degree = 1:4, nprune = seq(2, 50, by = 5)))

# Set seed for reproducibility
set.seed(2020)

# Perform cross-validation
cv_mars_audit <- train(x = complete_train_audit_m[, 2:5], 
                       y = complete_train_audit_m[, 10], 
                       method = "earth",
                       metric = "RMSE",
                       trControl = trainControl(method = "cv", number = 10),
                       tuneGrid = parameter_grid)
# Save the results 
save(cv_mars_audit, file = "cv_mars_audit.RDa")
```

```{r}
# Load package
library(earth)

# Load files 
load("cv_mars_audit.RDa")

# Visualize the results from the cross validation on the parameter tuning 
ggplot(cv_mars_audit) + 
  labs(x = "Value of nprune, the max # of predictors allowed", 
       fill = "Value of degree, max degree of interaction allowed") +
  ggtitle("Parameter Tuning for MARS")
```

We can see that the root mean squared error is minimized when `degree` is 1 and `nprune` is 2. For all values of these parameters, we can see that this quantity is very high. When we consider that the largest value of `RISK_Adjustment` in our training set (our response variable) is 99999, we see that this value of RMSE is extremely high and suggests that our model does not fit the training data very well. 

```{r}
# Use the best model to predict RISK_Adjustment for the test set 
mars_predict <- predict(cv_mars_audit$finalModel, newdata = complete_test_audit_m)

# Get the SSE 
mars_sse <- sum((complete_test_audit_m$RISK_Adjustment - mars_predict)^2)

# Get the mean absolute error 
mars_mae <- mean(abs(complete_test_audit_m$RISK_Adjustment - mars_predict))
```

We can use our model to predict the response for the test set. Doing this, we obtain an SSE of roughly 30.8 billion (which is extremely high and is slightly higher than what we obtained with OLS). On average, this works out to an error of roughly $2850. It is possible that we overfit our training data.

Below, we plot the residuals, and observe that the assumptions for linear regression are very clearly violated. In order for the MARS algorithm to be successful at modelling non-linear data, the data still must be linear enough in small sections that piecewise linear regression would be useful. This is **not** the case with our data, as evidenced by our residual plot.

```{r}
plot(complete_test_audit_m$RISK_Adjustment - mars_predict, 
     main = "Residuals for MARS model", 
     xlab = "Observation number in test set", 
     ylab = "Residual")
```

### Regression Trees

Regression trees are very similar to classification trees, with the exception that they aim to predict the value of a continuous variable rather than a binary variable. With regression trees, there are only specific values which may be predicted. These possible values are visible in the leaf nodes (bottom) of the tree.

First, we use the `tree` package, trying both the `deviance` and `gini` options. The `deviance` option results in a tree with 5 leaf nodes, but the `gini` option results in a tree which has only has a root. This indicates that the data may not be well suited to regression trees. The tree below shows the regression tree obtained using the `deviance` option in the `tree` function: 

```{r, message = FALSE}
# Use deviance:
# Note: mincut is the minimum number of observations per node
reg_tree_dev <- tree(formula = RISK_Adjustment ~ ., data = train_audit_m,
                     split = "deviance")

# Use generalized gini
reg_tree_gini <- tree(formula = RISK_Adjustment ~ ., data = train_audit_m,
                      split = "gini")

# Visualize the results 
plot(reg_tree_dev)
text(reg_tree_dev, all = TRUE)
# The gini tree will not plot, as it is only a root 
# plot(reg_tree_gini)
# text(reg_tree_gini, all = TRUE)

# Calculate prediction accuracy on the test set 
reg_tree_dev_predict <- predict(reg_tree_dev, test_audit_m)
reg_tree_gini_predict <- predict(reg_tree_gini, test_audit_m)
reg_tree_dev_sse <- sum((test_audit_m$RISK_Adjustment - reg_tree_dev_predict)^2)
reg_tree_gini_sse <- sum((test_audit_m$RISK_Adjustment - reg_tree_gini_predict)^2)
```

We can also use the `rpart` package to create regression trees. 

```{r}
# Create decision tree
# Note: minsplit is the minimum number of observations per node
reg_tree_rpart <- rpart(formula = RISK_Adjustment ~ ., data = train_audit_m)

# Calculate prediction accuracy on the test set 
reg_tree_rpart_predict <- predict(reg_tree_rpart, test_audit_m)
reg_tree_rpart_sse <- sum((test_audit_m$RISK_Adjustment - reg_tree_rpart_predict)^2)

# Plot the tree
fancyRpartPlot(reg_tree_rpart, sub = "Regression tree created using the rpart package")
```

We can see that the `rpart` tree had 6 leaf nodes.

For the `tree` package, the regression tree created using the `deviance` option had an SSE of approximately 30.8 billion when predicting the response variable on the test set, while it was 32.6 billion for the tree created using the `gini` option. This is not surprising to see a large difference between the two values, since we know that the second tree was simply predicting the same value regardless of the input. For the regression tree created using the `rpart` package, the SSE on the test set was ~32.7 billion. 

The plot below shows the residuals for the regression tree created using the `tree` package with the `deviance` option.

```{r}
plot(test_audit_m$RISK_Adjustment - reg_tree_dev_predict, 
     main = "Residuals for Regression Tree (Deviance)", 
     xlab = "Observation number in test set", 
     ylab = "Residual")
```

We can see that the residuals once again indicate that our model was not well suited to our data. 

While we cannot visualize in as many dimensions as there are in our dataset, we can choose one variable in our dataset and plot it again the response variable for both the true values and the predicted values using the regression tree: 

```{r}
plot(test_audit_m$Age, test_audit_m$RISK_Adjustment, pch = 19,
     main = "Regression Tree Predictions on Test Set (Using 'Deviance')", 
     xlab = "Normalized Age",
     ylab = "Risk Adjustment")
points(test_audit_m$Age, reg_tree_dev_predict, pch = 19, col = "red")
legend(legend = c("True", "Predicted"), col = c("Black", "Red"), pch = c(19, 19), 
       x = "topleft")
```

From the plot, we can see that our regression tree is failing to predict high risk adjustments. It is also failing to predict any risk adjustments of exactly 0.

### Zero-Inflated Regression

*Zero-Inflated Regression* is a popular technique for predicting response variables that are over saturated with the value 0. With zero-inflated regression models, the assumption is made that the zeroes come from one distribution, while the non-zero values come from another distribution (this other distribution may also generate zeroes occasionally). Thus, zero-inflated regression "mixes" models together. 

For count data, zero-inflated Poisson models are used frequently. A classic example of data coming from a zero-inflated Poisson distribution is the number of fish caught by visitors in a wildlife park. Many visitors may not fish at all, so there are many zeroes in the data. Some visitors may spend their entire visit fishing, and the distribution for the number of fish caught by these visitors will follow a Poisson distribution. 

For the audit data, the response variable `RISK_Adjustment` can be thought of as a zero-inflated variable. This is because most of the individuals in the dataset do not have an adjustment made (resulting in many zeroes), but the adjustments that are non-zero follow some (unknown) distribution. We can create a histogram of the non-zero risk adjustments to understand the distribution a bit better: 

```{r}
x_vals <- seq(0, 30000, by = 1)
nonzero_adjustments <- audit$RISK_Adjustment[which(audit$RISK_Adjustment > 0)]
hist(nonzero_adjustments, probability = TRUE, 
     main = "Histogram of Non-Zero Risk Adjustments",
     xlab = "Value")
```

The data look like they could be Poisson, but there are a handful of outliers. For zero-inflated count data where the variance is much larger than the mean, it is common to use zero-inflated negative binomial regression. The `pscl` package in R will allow us to perform these two types of zero-inflated regression. Since the variance of our non-zero risk adjustments is larger than the mean of our non-zero risk adjustments, we expect that the Poisson will not perform as well as the other option. 

Note that since there are a few risk adjustments which are negative and the above distributions are positive, we will temporarily assign negative adjustments to the value 0. We will also only use observations without any missing values, as this is required by the `zeroinfl` function. The results from the zero-inflated regression are shown in the table below: 

```{r}
# Load library for performing zero-inflated regression
library(pscl)

# If response is less than 0, assign to zero temporarily 
pos_train_audit <- complete_train_audit_m
pos_test_audit <- complete_test_audit_m 
pos_train_audit$RISK_Adjustment <- ifelse(pos_train_audit$RISK_Adjustment < 0, 
                                           0, pos_train_audit$RISK_Adjustment)
pos_test_audit$RISK_Adjustment <- ifelse(pos_test_audit$RISK_Adjustment < 0, 
                                           0, pos_test_audit$RISK_Adjustment)


# Perform zero-inflated poisson regression (ZIP)
zip <- zeroinfl(formula = RISK_Adjustment ~ ., data = pos_train_audit, dist = "poisson")
zip_predict <- predict(zip, newdata = pos_test_audit)
zip_sse <- sum((complete_test_audit_m$RISK_Adjustment - zip_predict)^2)
zip_mae <- mean(abs(complete_test_audit_m$RISK_Adjustment - zip_predict))

# Perform zero-inflated negative binomial regression
zi_negbin <- zeroinfl(formula = RISK_Adjustment ~ ., data = pos_train_audit, dist = "negbin")
zi_negbin_predict <- predict(zi_negbin, newdata = pos_test_audit)
zi_negbin_sse <- sum((complete_test_audit_m$RISK_Adjustment - zi_negbin_predict)^2)
zi_negbin_mae <- mean(abs(complete_test_audit_m$RISK_Adjustment - zi_negbin_predict))

# Combine results into a single data frame
results <- data.frame(SSE = c(zip_sse, zi_negbin_sse), 
                      MAE = c(zip_mae, zi_negbin_mae))
row.names(results) <- c("Zero-Inflated Poisson", "Zero-Inflated Negative Binomial")

# Display results
kable(results) %>% 
  kable_styling(latex_options = "striped")
```

We can see that the negative binomial performed better than the Poisson, as expected.

We can also compare our above results to results obtained using standard GLMs with the same distributions (i.e., not using zero-inflated models) for comparison: 

```{r}
# Load library for performing negative binomial regression
library(MASS)

# Poisson regression
pois_glm <- glm(formula = RISK_Adjustment ~ ., data = pos_train_audit, 
                family = "poisson", na.action = "na.omit")
pois_glm_predict <- predict(pois_glm, newdata = pos_test_audit)
pois_glm_sse <- sum((complete_test_audit_m$RISK_Adjustment - pois_glm_predict)^2)
pois_glm_mae <- mean(abs(complete_test_audit_m$RISK_Adjustment - pois_glm_predict))

# Negative binomial regression
negbin_glm <- glm.nb(formula = RISK_Adjustment ~ ., data = pos_train_audit, 
                     na.action = "na.omit")
negbin_glm_predict <- predict(negbin_glm, newdata = pos_test_audit)
negbin_glm_sse <- sum((complete_test_audit_m$RISK_Adjustment - negbin_glm_predict)^2)
negbin_glm_mae <- mean(abs(complete_test_audit_m$RISK_Adjustment - negbin_glm_predict))

# Combine results into a single data frame
results <- data.frame(SSE = c(pois_glm_sse, negbin_glm_sse), 
                      MAE = c(pois_glm_mae, negbin_glm_mae))
row.names(results) <- c("Poisson GLM", 
                    "Negative Binomial GLM")

# Display results
kable(results) %>% 
  kable_styling(latex_options = "striped")
```

We can see that, in both cases, the zero-inflated models resulted in much smaller values for the SSE. The mean absolute errors were lower with the standard GLMs. 

### Neural Networks

Lastly, we use neural networks to predict the `RISK_Adjustment` response variable. We will follow the same methods as we did when we used neural networks for classification (tuning parameters via the `ANN2` package). The boxplots below show the SSEs on the test set for the various parameter combinations, grouped by optimization technique. 

```{r, eval = FALSE}
# Set seed for reproducibility
set.seed(24)

# Convert categorical variables to one-hot encoded variables
one_hot_train <- model.matrix(object = RISK_Adjustment ~ . - 1, 
                                data = complete_train_audit_m)
one_hot_test <- model.matrix(object = RISK_Adjustment ~ . - 1, 
                             data = complete_test_audit_m)

# Create a parameter tuning "grid" 
parameter_grid <- expand.grid(activ.functions = c("tanh", "sigmoid", "relu", "linear"),
                              optim.type = c("sgd", "rmsprop", "adam"),
                              L2 = seq(0, 1, by = 0.1))

nn_sse <- vector(length = nrow(parameter_grid))
nn_mae <- vector(length = nrow(parameter_grid))
for (i in 1:nrow(parameter_grid)) {
  # Create neural network
  audit_nn <- neuralnetwork(X = one_hot_train, 
                            y = complete_train_audit_m[, 10], 
                            hidden.layers = NA, 
                            loss.type = "squared", 
                            regression = TRUE,
                            activ.functions = as.character(parameter_grid$activ.functions[i]),
                            optim.type = as.character(parameter_grid$optim.type[i]), 
                            L2 = parameter_grid$L2[i])
  
  nn_predict <- predict(audit_nn, newdata = one_hot_test)$predictions[, 1]
  
  # Get the SSE 
  nn_sse[i] <- sum((complete_test_audit_m$RISK_Adjustment - nn_predict)^2)

  # Get the mean absolute error 
  nn_mae[i] <- mean(abs(complete_test_audit_m$RISK_Adjustment - nn_predict))
}

# Save results
save(nn_sse, file = "nn_sse.RDa")
save(nn_mae, file = "nn_mae.RDa")

# Find parameters for the best model
best_params <- parameter_grid[85, ]

# Set seed for reproducibility 
set.seed(94)

# Play around with more neurons 
num_neurons <- seq(2, 64, by = 1)
nn_sse_2 <- vector(length = length(num_neurons))
nn_mae_2 <- vector(length = length(num_neurons))
for (i in 1:6) {
  # Create neural network
  audit_nn <- neuralnetwork(X = one_hot_train, 
                            y = complete_train_audit_m[, 10], 
                            hidden.layers = num_neurons[i], 
                            loss.type = "squared", 
                            regression = TRUE,
                            activ.functions = as.character(best_params$activ.functions),
                            optim.type = as.character(best_params$optim.type), 
                            L2 = best_params$L2)

  nn_predict <- predict(audit_nn, newdata = one_hot_test)$predictions[, 1]
  
  # Get the SSE 
  nn_sse_2[i] <- sum((complete_test_audit_m$RISK_Adjustment - nn_predict)^2)

  # Get the mean absolute error 
  nn_mae_2[i] <- mean(abs(complete_test_audit_m$RISK_Adjustment - nn_predict))
}

# Save results
save(nn_sse_2, file = "nn_sse_2.RDa")
save(nn_mae_2, file = "nn_mae_2.RDa")
```

```{r}
# Load results 
load("nn_sse.RDa")
load("nn_mae.RDa")

# Create data frame of results from varying parameters (in order to plot with ggplot)
nn_results <- data.frame(SSE = nn_sse, MAE = nn_mae, 
                         optim.type = rep(rep(c("sgd", "rmsprop", "adam"), each = 4), 11))

# Create boxplot of SSE, grouped by optimization algorithm
ggplot(data = nn_results, aes(y = SSE, fill = optim.type)) +
  geom_boxplot() + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

We notice that stochastic gradient descent performed best on average. We'll now select the lowest SSE in the stochastic gradient descent category (hyperbolic tangent activation function with $\lambda = 0.7$ for L2 regularization) and add a hidden layer, experimenting with the number of hidden neurons. The results are shown below.  

```{r}
# Load results
load("nn_sse_2.RDa")
load("nn_mae_2.RDa")

# Create data frame of results from varying number of hidden neurons (in order to use ggplot)
nn_results <- data.frame(value = c(nn_sse_2, nn_mae_2), 
                         type = as.factor(rep(c("SSE", "MAE"), each = length(nn_sse_2))), 
                         num_neurons = rep(seq(2, 64, by = 1), 2))

nn_results <- data.frame(SSE = nn_sse_2, MAE = nn_mae_2,
                         num_neurons = seq(2, 64, by = 1))

# Create line plot of SSE and MAE as a function of the number of hidden neurons
ggplot(data = nn_results, aes(x = num_neurons)) + 
  geom_line(aes(x = num_neurons, y = SSE, colour = I("#F8766D"), alpha = I(0.8))) + 
  geom_line(aes(x = num_neurons, y = MAE * 9.4e6, colour = I("#00BFC4"), alpha = I(0.5))) + 
  scale_y_continuous(name = "SSE", sec.axis = sec_axis(~ . / 9.4e6, name = "MAE")) + 
  theme(axis.line.y.right = element_line(color = "#00BFC4"), 
        axis.ticks.y.right = element_line(color = "#00BFC4"),
        axis.text.y.right = element_text(color = "#00BFC4"), 
        axis.title.y.right = element_text(color = "#00BFC4"),
        axis.line.y.left = element_line(color = "#F8766D"), 
        axis.ticks.y.left = element_line(color = "#F8766D"),
        axis.text.y.left = element_text(color = "#F8766D"), 
        axis.title.y.left = element_text(color = "#F8766D")) +
  labs(x = "Number of neurons in hidden layer") + 
  ggtitle("Performance on Test Data as a Function of Number of Hidden Neurons")
```

The minimum SSE was achieved with 7 neurons in the hidden layer. The value is 29.05 billion, which is the smallest SSE out of all the methods we have tried. The minimum MAE was reached at a different number of hidden neurons, but was a fairly comparable value (~$3250) to the other methods we tried. 

# References

https://rstudio-pubs-static.s3.amazonaws.com/202050_60f5c6dd1b884ba3ac29f1aef5c848bb.html






